diff --git a/MUJOCO_LOG.TXT b/MUJOCO_LOG.TXT
new file mode 100644
index 0000000..9b0ca48
--- /dev/null
+++ b/MUJOCO_LOG.TXT
@@ -0,0 +1,6 @@
+Tue Jun  7 15:18:18 2022
+ERROR: GLEW initalization error: Missing GL version
+
+Tue Jun  7 15:18:31 2022
+ERROR: GLEW initalization error: Missing GL version
+
diff --git a/Residual-Policy-Learning/MUJOCO_LOG.TXT b/Residual-Policy-Learning/MUJOCO_LOG.TXT
new file mode 100644
index 0000000..0bf1374
--- /dev/null
+++ b/Residual-Policy-Learning/MUJOCO_LOG.TXT
@@ -0,0 +1,6 @@
+Tue Jun  7 15:13:52 2022
+ERROR: GLEW initalization error: Missing GL version
+
+Tue Jun  7 15:15:04 2022
+ERROR: GLEW initalization error: Missing GL version
+
diff --git a/Residual-Policy-Learning/RL/baseline.py b/Residual-Policy-Learning/RL/baseline.py
index 538d61f..ec95a51 100644
--- a/Residual-Policy-Learning/RL/baseline.py
+++ b/Residual-Policy-Learning/RL/baseline.py
@@ -79,7 +79,10 @@ class Agent:
             obs = observation['observation']
             g = observation['desired_goal']
             for i in range(self.env_params['max_timesteps']):
-                actions = np.array([0,0,0,0])
+                if 'Nut' in args.env_name:
+                    actions = np.array([0,0,0,0,0,0,0])
+                else:
+                    actions = np.array([0,0,0,0])
                 observation_new, _, _, info = self.env.step(actions)
                 obs = observation_new['observation']
                 g = observation_new['desired_goal']
@@ -131,7 +134,7 @@ if __name__ == "__main__":
         wandb_save_dir = os.path.join(os.path.abspath(os.getcwd()),f"wandb_{args.env_name}")
         if not os.path.exists(wandb_save_dir):
             os.makedirs(wandb_save_dir)
-        wandb.init(project='Residual Policy Learning', entity='6-881_project',\
+        wandb.init(project='Residual Policy Learning', entity='turbohiro',\
                    sync_tensorboard=True, config=vars(args), name=experiment_name,\
                    save_code=True, dir=wandb_save_dir, group=f"{args.env_name}")
         writer = SummaryWriter(f"{wandb.run.dir}/{experiment_name}")
diff --git a/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py b/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
index 8d63c3f..081e342 100644
--- a/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
+++ b/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
@@ -57,7 +57,7 @@ class NutAssembly(gym.Env):
         ob, reward, done, info = self.env.step(action)
         ob = self.env.observation_spec()
         observation = {}
-        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut0_pos'], ob['RoundNut0_quat']))
+        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
         observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
         observation['achieved_goal'] = ob['RoundNut_pos']
         info['is_success'] = reward
@@ -67,7 +67,7 @@ class NutAssembly(gym.Env):
         ob = self.env.reset()
         ob = self.env.observation_spec()
         observation = {}
-        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut0_pos'], ob['RoundNut0_quat']))
+        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
         observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
         observation['achieved_goal'] = ob['RoundNut_pos']
         return observation
@@ -229,7 +229,7 @@ class NutAssemblyHand(gym.Env):
             action_angle = [0,0,action_angle[-1]]
             
             #print('!!!!!!&&&',action_angle[-1],self.gripper_p,self.nut_p)
-            if np.linalg.norm(object_axang_touse - changing_wf_axang) <0.1 or self.ori_adjust_num>5:
+            if np.linalg.norm(object_axang_touse - changing_wf_axang) <0.1 or self.ori_adjust_num>8:
                 if take_action:
                     self.gripper_reoriented = 5
                     #print('successful action angle is:',action_angle)
@@ -261,6 +261,181 @@ class NutAssemblyHand(gym.Env):
         action = np.clip(action, -1, 1)
         return action
 
+class NutAssemblyHandSticky(gym.Env):
+    """
+        NutAssemblyHand:
+            'NutAssemblyHand' with an imperfect controller
+            Pose control for controller mode in robosuite
+            Action taken as:
+                Pi_theta(s) = pi_theta(s) + f(s)
+        Parameters:
+        -----------
+        kp: float
+            Scaling factor for position control
+    """
+    def __init__(self, kp:float=20, *args, **kwargs):
+        options = {}
+        controller_name = 'OSC_POSE'
+        options["controller_configs"] = suite.load_controller_config(default_controller=controller_name)
+
+        self.env = GymWrapper(
+            suite.make(
+                "NutAssemblyRound",             # Nut Assembly task with the round peg
+                robots="IIWA",                  # use IIWA robot
+                **options,                      # controller options
+                use_object_obs = True,
+                use_camera_obs=False,           # do not use pixel observations
+                has_offscreen_renderer=False,   # not needed since not using pixel obs
+                has_renderer=False,              # make sure we can render to the screen
+                reward_shaping=False,            # use sparse rewards
+                control_freq=20,                # control should happen fast enough so that simulation looks smooth
+            )
+        )
+
+        self.max_episode_steps = 500
+        self.action_space = self.env.action_space
+        self.observation_space = self.env.observation_space
+        self.active_observables = self.env.active_observables
+        self.modalities = self.env.observation_modalities
+        self.reward_type = 'sparse'
+        self.distance_threshold = 0.065
+        self.sticky_prob = 0.5
+
+    def step(self, residual_action:np.ndarray):
+        controller_action = np.array(self.controller_action(self.last_observation))
+        if (controller_action>1).any() or (controller_action<-1).any():
+            print(controller_action)
+        action = np.add(controller_action, residual_action)
+        action = np.clip(action, -1, 1)
+        ob, reward, done, info = self.env.step(action)
+        ob = self.env.observation_spec()
+        observation = {}
+        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
+        observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
+        observation['achieved_goal'] = ob['RoundNut_pos']
+        self.last_observation = observation.copy()
+        info['is_success'] = reward
+        return observation, reward, done, info
+
+    def reset(self):
+        self.env.reset() # reset according to task defaults
+        ob = self.env.observation_spec()
+        observation = {}
+        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
+        observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
+        observation['achieved_goal'] = ob['RoundNut_pos']
+        self.last_observation = observation.copy()
+        self.object_in_hand = False
+        self.object_below_hand = False
+        self.gripper_reoriented = 0
+        self.ori_adjust_num = 0
+        self.prev_action = [0,0,0,0,0,0,0]
+       
+        return observation
+
+    def seed(self, seed:int=0):
+        self.np_random, seed = seeding.np_random(seed)
+        return self.env.seed(seed=seed)
+
+    def render(self, mode:str="human",**kwargs):
+        return self.env.render(**kwargs)
+
+    def close(self):
+        return self.env.close()
+
+    def goal_distance(self, achieved_goal, desired_goal):
+        return np.linalg.norm(achieved_goal-desired_goal, axis = 1)
+
+    def compute_reward(self, achieved_goal, goal, info):
+        # Compute distance between goal and the achieved goal.
+        d = self.goal_distance(achieved_goal, goal)
+        if self.reward_type == 'sparse':
+            return -(d > self.distance_threshold).astype(np.float32)
+        else:
+            return -d
+
+    def controller_action(self, obs:dict, take_action:bool=True, DEBUG:bool=False):
+        observation = obs['observation']
+        goal_pos = obs['desired_goal']
+        achieved_goal = obs['achieved_goal']
+
+        gripper_pos = observation[:3]
+        gripper_quat = observation[3:7]
+        object_pos  = observation[7:10]
+        object_quat = observation[10:]
+
+        z_table = 0.8610982     # the z-coordinate of the table surface
+        
+        object_axang = T.quat2axisangle(object_quat)
+        #print('&&&&&&&&&&',object_axang[-1])
+        #if abs(object_axang[-1] - 1.24) < 0.2:
+        #    object_axang_touse = [0,0,object_axang[-1]%(2*pi/8) + (2*pi/8)]
+        #else:
+        #    object_axang_touse = [0,0,object_axang[-1]%(2*pi/8)]
+        object_axang_touse = [0,0,object_axang[-1]]
+        gripper_axang = T.quat2axisangle(gripper_quat)
+        #print('!!!!!!',gripper_axang[-1])
+
+        if self.gripper_reoriented == 0:
+            self.gripper_init_quat = gripper_quat
+            self.gripper_reoriented = 1
+
+        init_inv = T.quat_inverse(self.gripper_init_quat)
+        changing_wf = T.quat_multiply(init_inv,gripper_quat)
+        changing_wf_axang = T.quat2axisangle(changing_wf)
+        #print('!!!!!!&&&',changing_wf_axang[-1])
+        
+        # reorient the gripper to match the nut faces and move above the nut
+        if not self.object_below_hand or self.gripper_reoriented < 5:
+            self.nut_p = T.quat2axisangle(object_quat)[-1]
+            self.gripper_p = T.quat2axisangle(gripper_quat)[-1]
+            if not self.object_below_hand:
+                action = 20 * (object_pos[:2] - gripper_pos[:2])
+            else:
+                action = [0,0]
+
+            action = 20 * (object_pos[:2] - gripper_pos[:2])
+
+            action_angle = 0.2*(object_axang_touse - changing_wf_axang)
+            action_angle = [0,0,action_angle[-1]]
+            
+            #print('!!!!!!&&&',action_angle[-1],self.gripper_p,self.nut_p)
+            if np.linalg.norm(object_axang_touse - changing_wf_axang) <0.1 or self.ori_adjust_num>8:
+                if take_action:
+                    self.gripper_reoriented = 5
+                    #print('successful action angle is:',action_angle)
+                    
+            #action_angle =[0, 0, 0]
+            self.ori_adjust_num +=1
+            #print(self.ori_adjust_num)
+            action = np.hstack((action, [0], action_angle, [-1]))
+            if np.linalg.norm((object_pos[:2] - gripper_pos[:2])) < 0.01:
+                if take_action:
+                    self.object_below_hand = True
+                    #self.gripper_reoriented = 5
+        # close the gripper and pick the nut
+        elif not self.object_in_hand: # Close gripper
+            action = [0,0,-1,0,0,0,-1]
+            if np.linalg.norm((object_pos[2] - gripper_pos[2])) < 0.01:
+                action = [0,0,0,0,0,0,1]
+                if take_action:
+                    self.object_in_hand = True
+        
+        else: # Move gripper up and toward goal position
+            action = [0,0,1,0,0,0,1]
+            if object_pos[2] - z_table > 0.1:
+                action = 20 * (goal_pos[:2] - object_pos[:2])
+                action = np.hstack((action,[0,0,0,0,1]))
+                if np.linalg.norm((goal_pos[:2] - object_pos[:2])) < 0.0225:
+                    action = [0,0,0,0,0,0,-1] # Drop nut once it's close enough to the peg
+
+        if np.random.random() < self.sticky_prob:
+            action = np.array(self.prev_action)
+        else:
+            self.prev_action = action
+        
+        action = np.clip(action, -1, 1)
+        return action
 
 
 import pdb
diff --git a/Residual-Policy-Learning/run_assembly.sh b/Residual-Policy-Learning/run_assembly.sh
new file mode 100644
index 0000000..ebf75d4
--- /dev/null
+++ b/Residual-Policy-Learning/run_assembly.sh
@@ -0,0 +1,15 @@
+#!/bin/bash
+
+envs=("NutAssembly" "NutAssemblyHand" "NutAssemblyHandSticky")
+seeds=(1 2)
+exp_names=("rl" "res" "res")
+epochs=(500 500 500)
+# Run the script
+for i in ${!seeds[@]}; do
+    # run for different seeds
+    for j in ${!envs[@]}; do
+        # run for all different envs
+        echo "Env Name: ${envs[$j]} | Seed: ${seeds[$i]} | Exp Name: ${exp_names[$j]} | out_${envs[$j]}_${seeds[i]}"
+        mpirun python -m RL.ddpg.ddpg_mpi --env_name ${envs[$j]} --seed ${seeds[$i]} --n_epochs ${epochs[$j]}  --exp_name ${exp_names[$j]} 2>&1 | tee exp_outputs/out_${envs[$j]}_${seeds[i]}
+    done
+done
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/latest-run b/Residual-Policy-Learning/wandb_NutAssembly/wandb/latest-run
new file mode 120000
index 0000000..8c58ca3
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/latest-run
@@ -0,0 +1 @@
+run-20220608_105655-3gvz6g4e
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py
new file mode 100644
index 0000000..33b7b76
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py
@@ -0,0 +1,504 @@
+"""
+    DDPG with HER
+"""
+import copy
+import gym
+import argparse
+import torch
+from torch import nn
+from torch import optim
+import os
+from datetime import datetime
+import numpy as np
+from typing import Tuple
+
+from RL.ddpg.models import actor, critic
+from RL.ddpg.replay_buffer import replay_buffer
+from her.her import her_sampler
+
+OPTIMIZERS = {
+    'adam': optim.Adam,
+    'adamax': optim.Adamax,
+    'rmsprop': optim.RMSprop,
+}
+
+LOSS_FN = {
+    'mse': nn.MSELoss(),
+    'smooth_l1': nn.SmoothL1Loss(),
+    'l1': nn.L1Loss()
+}
+
+class DDPG_Agent:
+    def __init__(self, args:argparse.Namespace, env, save_dir: str, device:str, writer=None):
+        """
+            Module for the DDPG agent along with HER
+            Parameters:
+            -----------
+            args: argparse.Namespace
+                args should contain the following:
+            env: gym.Env
+                OpenAI type gym environment
+            save_dir: str
+                Path to save the network weights, checkpoints
+            device: str
+                device to run the training process on
+                Choices: 'cpu', 'cuda'
+            writer: tensorboardX
+                tensorboardX to log metrics like losses, rewards, success_rates, etc.
+        """
+        self.args = args
+        self.env = env
+        self.env_params = self.get_env_params(env)
+        self.save_dir = save_dir
+        self.device = device
+        self.writer = writer
+        self.sim_steps = 0      # a count to keep track of number of simulation steps
+        self.train_steps = 0    # a count to keep track of number of training steps
+
+        # create the network
+        self.actor_network = actor(args, self.env_params).to(device)
+        self.critic_network = critic(args, self.env_params).to(device)
+        # build up the target network
+        self.actor_target_network = actor(args, self.env_params).to(device)
+        self.critic_target_network = critic(args, self.env_params).to(device)
+
+        # load the weights into the target networks
+        self.actor_target_network.load_state_dict(self.actor_network.state_dict())
+        self.critic_target_network.load_state_dict(self.critic_network.state_dict())
+
+        # create the optimizer
+        self.actor_optim  = OPTIMIZERS[args.actor_optim](self.actor_network.parameters(),\
+                                                         lr=self.args.actor_lr,\
+                                                         weight_decay=self.args.weight_decay)
+        self.critic_optim = OPTIMIZERS[args.critic_optim](self.critic_network.parameters(),
+                                                         lr=self.args.critic_lr,\
+                                                         weight_decay=self.args.weight_decay)
+        self.burn_in_done = False   # to check if burn-in is done or not
+
+        # loss function for DDPG
+        self.criterion = LOSS_FN[args.loss_fn]
+
+        # her sampler
+        self.her_module = her_sampler(replay_strategy = self.args.replay_strategy, \
+                                      replay_k = self.args.replay_k, \
+                                      reward_func = self.env.compute_reward)
+        # create the replay buffer
+        self.buffer = replay_buffer(self.env_params, \
+                                    self.args.buffer_size, \
+                                    self.her_module.sample_her_transitions)
+    
+    def get_env_params(self, env):
+        """
+            Get the environment parameters
+        """
+        obs = env.reset()
+        # close the environment
+        params = {'obs': obs['observation'].shape[0],
+                'goal': obs['desired_goal'].shape[0],
+                'action': env.action_space.shape[0],
+                'action_max': env.action_space.high[0],
+                }
+        try:
+            params['max_timesteps'] = env._max_episode_steps
+        # for custom envs
+        except:
+            params['max_timesteps'] = env.max_episode_steps
+        return params
+    
+    def set_actor_lr(self, loss:float, prev_loss:float ,verbose:bool=True):
+        """
+            Set the learning rate of the actor network
+            to either the original learning rate
+            or zero depending on the burn-in parameter
+            diff_loss = |loss - prev_loss|
+            if rl:
+                actor_lr = original_actor_lr
+            if residual learning and diff_loss < beta:
+                actor_lr = original_actor_lr
+            elif residual learning and diff_loss > beta:
+                actor_lr = 0
+            Parameters:
+            -----------
+            loss: float
+                Mean of the losses till the current epoch
+            prev_loss: float
+                Mean of losses till the last epoch
+            verbose: bool
+                To print the change in learning rate
+        """
+        lr = self.args.actor_lr
+        coin_flipping = False
+        # loss is zero only in the first epoch hence do not change lr then
+        # and that's why give a large value to diff_loss
+        diff_loss = 100 if loss == 0 else abs(loss - prev_loss)
+        if not self.burn_in_done:
+            if self.args.exp_name == 'res' and diff_loss > self.args.beta:
+                lr = 0.0
+                coin_flipping = True
+            elif self.args.exp_name == 'res' and diff_loss <= self.args.beta:
+                if verbose:
+                    print('_'*80)
+                    print(f'Burn-in of the critic done. Changing actor_lr from 0.0 to {self.args.actor_lr}')
+                    print('_'*80)
+                self.burn_in_done = True
+
+        for param_group in self.actor_optim.param_groups:
+            param_group['lr'] = lr
+        return coin_flipping
+        
+    def train(self):
+        """
+            Run the episodes for training
+        """
+        print('_'*50)
+        print('Beginning the training...')
+        print('_'*50)
+        num_cycles = 0
+        actor_losses = [0.0]    # to store actor losses for burn-in
+        critic_losses = [0.0]   # to store critic losses for burn-in
+        prev_losses = [0.0]
+        coin_flipping = False   # whether the whole episode should be noise and randomness free
+        deterministic = False   # choose whether we want deterministic or not  
+        for epoch in range(self.args.n_epochs):
+
+            # change the actor learning rate from zero to actor_lr by checking burn-in
+            # check config.py for more information on args.beta_monitor
+            if self.args.beta_monitor == 'actor':
+                coin_flipping = self.set_actor_lr(np.mean(actor_losses), np.mean(prev_losses))
+                prev_losses = actor_losses.copy()
+            elif self.args.beta_monitor == 'critic':
+                coin_flipping = self.set_actor_lr(np.mean(critic_losses), np.mean(prev_losses))
+                prev_losses = critic_losses.copy()
+
+            for cycle in range(self.args.n_cycles):
+
+                ep_obs, ep_ag, ep_g, ep_actions = [], [], [], []
+                # reset the environment
+                observation = self.env.reset()
+                observation_new = copy.deepcopy(observation)
+                obs = observation['observation']
+                ag = observation['achieved_goal']
+                g = observation['desired_goal']
+
+                random_eps = self.args.random_eps
+                noise_eps  = self.args.noise_eps
+                if coin_flipping:
+                    deterministic = np.random.random() < self.args.coin_flipping_prob  # NOTE/TODO change here
+                if deterministic:
+                    random_eps = 0.0
+                    noise_eps = 0.0
+
+                for t in range(self.env_params['max_timesteps']):
+                    # take actions 
+                    with torch.no_grad():
+                        state = self.preprocess_inputs(obs, g)
+                        pi = self.actor_network(state)
+                        if self.args.exp_name == 'res':
+                            controller_action = self.get_controller_actions(observation_new)
+                            action = self.select_actions(pi, noise_eps=noise_eps, random_eps= random_eps, controller_action=controller_action)
+                        else:
+                            action = self.select_actions(pi, noise_eps=noise_eps, random_eps= random_eps, controller_action=None)
+                    # give the action to the environment
+                    observation_new, _, _, info = self.env.step(action)
+                    self.sim_steps += 1                     # increase the simulation timestep by one
+                    obs_new = observation_new['observation']
+                    ag_new = observation_new['achieved_goal']
+                    # append rollouts
+                    ep_obs.append(obs.copy())
+                    ep_ag.append(ag.copy())
+                    ep_g.append(g.copy())
+                    ep_actions.append(action.copy())
+                    # re-assign the observation
+                    obs = obs_new
+                    ag = ag_new
+                # append last states in the array, extend the episode chain (state machine)
+                ep_obs.append(obs.copy())
+                ep_ag.append(ag.copy())
+                # convert to np arrays
+                ep_obs = np.expand_dims(np.array(ep_obs),0)
+                ep_ag = np.expand_dims(np.array(ep_ag),0)
+                ep_g = np.expand_dims(np.array(ep_g),0)
+                ep_actions = np.expand_dims(np.array(ep_actions),0)
+
+                # store them in buffer
+                self.buffer.store_episode([ep_obs, ep_ag, ep_g, ep_actions])
+
+                actor_loss_cycle = 0; critic_loss_cycle = 0 
+                for batch in range(self.args.n_batches):
+                    # train the network with 'n_batches' number of batches
+                    actor_loss, critic_loss = self.update_network(self.train_steps)
+                    self.train_steps += 1
+                    actor_losses.append(actor_loss)
+                    critic_losses.append(critic_loss)
+                    actor_loss_cycle += actor_loss
+                    critic_loss_cycle += critic_loss
+                if self.writer:
+                    self.writer.add_scalar('Cycle Losses/Actor Loss', actor_loss_cycle, num_cycles)
+                    self.writer.add_scalar('Cycle Losses/Critic Loss', critic_loss_cycle, num_cycles)
+                self.polyak_update_networks(self.actor_target_network, self.actor_network)
+                self.polyak_update_networks(self.critic_target_network, self.critic_network)
+                num_cycles += 1
+            # evaluate the agent
+            success_rate = self.eval_agent()
+            print(f'Epoch Critic: {np.mean(critic_losses):.3f} Epoch Actor:{np.mean(actor_losses):.3f}')
+            print(f'Epoch:{epoch}\tSuccess Rate:{success_rate:.3f}')
+            if self.writer:
+                self.writer.add_scalar('Success Rate/Success Rate', success_rate, self.sim_steps)
+                self.writer.add_scalar('Epoch Losses/Average Critic Loss', np.mean(critic_losses), epoch)
+                self.writer.add_scalar('Epoch Losses/Average Actor Loss', np.mean(actor_losses), epoch)
+            
+            self.save_checkpoint(self.save_dir)
+
+    def preprocess_inputs(self, obs:np.ndarray, g:np.ndarray) -> torch.Tensor:
+        """
+            Concatenate state and goal
+            and convert them to torch tensors
+            and then transfer them to either CPU of GPU
+        """
+        # concatenate the stuffs
+        inputs = np.concatenate([obs, g])   #observation input (state) and goal label (ground truth)
+        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)
+        inputs = inputs.to(self.device)
+        return inputs
+    
+    def get_controller_actions(self, obs:dict):
+        """
+            Return the controller action if residual learning
+        """
+        return self.env.controller_action(obs, take_action=False)
+
+    def select_actions(self, pi: torch.Tensor, noise_eps:float, random_eps:float, controller_action=None) -> np.ndarray:
+        """
+            Take action
+            with a probability of self.args.random_eps, it will take random actions
+            otherwise, this will add a gaussian noise to the action along with clipping
+            pi: torch.Tensor
+                The action given by the actor
+            noise_eps: float
+                The random gaussian noise added to actor output
+            random_eps: float
+                Probability of taking random action
+            controller_action: None or np.ndarray
+                To subtract from random action
+        """
+        # transfer action from CUDA to CPU if using GPU and make numpy array out of it
+        action = pi.cpu().numpy().squeeze()
+        # add the gaussian
+        action += noise_eps * self.env_params['action_max'] * np.random.randn(*action.shape)
+        action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])  #make action values are limited a feasible range
+
+        # random actions
+        random_actions = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \
+                                            size=self.env_params['action'])
+        # if residual learning, subtract the controller action so that we don't add it twice
+        if self.args.exp_name == 'res':
+            random_actions = random_actions - controller_action
+        # choose whether to take random actions or not
+        rand = np.random.binomial(1, random_eps, 1)[0]
+        action += rand * (random_actions - action)  # will be equal to either random_actions or action
+        return action
+
+    def preprocess_og(self, o:np.ndarray, g:np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
+        """
+            Perform observation clipping
+        """
+        o = np.clip(o, -self.args.clip_obs, self.args.clip_obs)
+        g = np.clip(g, -self.args.clip_obs, self.args.clip_obs)
+        return o, g
+
+    def polyak_update_networks(self, target, source):
+        """
+            Polyak averaging of target and main networks; Also known as soft update of networks
+            target_net_params = (1 - polyak) * main_net_params + polyak * target_net_params
+        """
+        for target_param, param in zip(target.parameters(), source.parameters()):
+            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)
+
+    def update_network(self, step:int) -> Tuple[float, float]:
+        """
+            The actual DDPG training
+        """
+
+        # sample the episodes
+        transitions = self.buffer.sample(self.args.batch_size)
+
+        # pre-process the observation and goal
+        o, o_next, g = transitions['obs'], transitions['obs_next'], transitions['g']
+        transitions['obs'], transitions['g'] = self.preprocess_og(o, g)
+        transitions['obs_next'], transitions['g_next'] = self.preprocess_og(o_next, g)
+
+        # concatenate obs and goal
+        states = np.concatenate([transitions['obs'], transitions['g']], axis=1)
+        next_states = np.concatenate([transitions['obs_next'], transitions['g_next']], axis=1)
+
+        # convert to tensor
+        states = torch.tensor(states, dtype=torch.float32).to(self.device)
+        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)
+        actions = torch.tensor(transitions['actions'], dtype=torch.float32).to(self.device)
+        rewards = torch.tensor(transitions['r'], dtype=torch.float32).to(self.device)
+
+        with torch.no_grad():
+            actions_next = self.actor_target_network(next_states)
+            q_next_value = self.critic_target_network(next_states, actions_next)
+            q_next_value = q_next_value.detach()
+            target_q_value = rewards + self.args.gamma * q_next_value
+            target_q_value = target_q_value.detach()
+            # clip the returns
+            clip_return = 1 / (1-self.args.gamma)
+            target_q_value = torch.clamp(target_q_value, -clip_return, 0)
+        
+        # critic loss
+        q_value = self.critic_network(states, actions)
+        critic_loss = self.criterion(target_q_value, q_value)   # loss (mostly MSE)
+
+        # actor loss
+        actions_pred = self.actor_network(states)
+        actor_loss = -self.critic_network(states, actions_pred).mean()   #the max of q values, the better of actor(policy)
+        actor_loss = actor_loss + self.args.action_l2 * (actions_pred / self.env_params['action_max']).pow(2).mean()
+
+        # backpropagate
+        self.actor_optim.zero_grad()    # zero the gradients
+        actor_loss.backward()           # backward prop
+        self.actor_optim.step()         # take step towards gradient direction
+
+        self.critic_optim.zero_grad()    # zero the gradients
+        critic_loss.backward()           # backward prop
+        self.critic_optim.step()         # take step towards gradient directions
+
+        return actor_loss.item(), critic_loss.item()
+        
+    def eval_agent(self) -> float:
+        """
+            Evaluate the agent using the trained policy
+            performs n_test_rollouts in the environment
+            and returns
+        """
+        successes = []
+        for _ in range(self.args.n_test_rollouts):
+            success = np.zeros(self.env_params['max_timesteps'])
+            observation = self.env.reset()
+            obs = observation['observation']
+            g = observation['desired_goal']
+            for i in range(self.env_params['max_timesteps']):
+                with torch.no_grad():
+                    input_tensor = self.preprocess_inputs(obs,g)
+                    pi = self.actor_network(input_tensor)
+                    actions = pi.detach().cpu().numpy().squeeze()
+                observation_new, _, _, info = self.env.step(actions)
+                obs = observation_new['observation']
+                g = observation_new['desired_goal']
+                success[i] = info['is_success']
+            successes.append(success)
+        successes = np.array(successes)
+        return np.mean(successes[:,-1]) # return mean of only final steps success
+
+    def save_checkpoint(self, path:str):
+        """
+            Saves the model in the wandb experiment run directory
+            This will store the 
+                * model state_dict
+                * optimizer state_dict
+                * args/hparams
+            param:
+                path: str
+                    path to the wandb run directory
+                    Example: os.path.join(wandb.run.dir, "model.ckpt")
+        """
+        checkpoint = {}
+        checkpoint['args'] = vars(self.args)
+        checkpoint['actor_state_dict'] = self.actor_network.state_dict()
+        checkpoint['env_name'] = self.args.env_name
+        torch.save(checkpoint, path)
+
+    def load_checkpoint(self, path:str):
+        """
+            Load the trained model weights
+            param:
+                path: str
+                    path to the saved weights file
+        """
+        use_cuda = torch.cuda.is_available()
+        device   = torch.device("cuda" if use_cuda else "cpu")
+        checkpoint_dict = torch.load(path, map_location=device)
+        self.actor_network.load_state_dict(checkpoint_dict['actor_state_dict'])
+
+if __name__ == "__main__":
+    # run from main folder as `python RL/ddpg.py`
+    import time
+    import wandb
+    from torch.utils.tensorboard import SummaryWriter
+
+    from ddpg_config import args
+    from utils import connected_to_internet, make_env, get_pretty_env_name
+    import pdb
+
+    # check whether GPU is available or not
+    use_cuda = torch.cuda.is_available()
+    device = torch.device("cuda" if use_cuda else "cpu")
+    print('_'*50)
+    print('Device:',device)
+    print('_'*50)
+    args.device = device
+    args.mpi = False        # not running on mpi mode
+    #####################################
+
+    env = make_env(args.env_name)   # initialise the environment
+    #env = gym.make(args.env_name)   # initialise the environment
+    
+    # set the random seeds for everything
+    # random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+    env.seed(args.seed)
+    env.action_space.seed(args.seed)
+    env.observation_space.seed(args.seed)
+    #####################################
+
+    # book keeping to log stuff
+    if args.dryrun:
+        writer = None
+        weight_save_path = 'model_dryrun.ckpt'
+    else:
+        # check internet connection
+        # for offline wandb. Will load everything on cloud afterwards
+        if not connected_to_internet():
+            import json
+            # save a json file with your wandb api key in your home folder as {'my_wandb_api_key': 'INSERT API HERE'}
+            # NOTE this is only for running on MIT Supercloud
+            with open(os.path.expanduser('~')+'/keys.json') as json_file: 
+                key = json.load(json_file)
+                my_wandb_api_key = key['my_wandb_api_key'] # NOTE change here as well
+            os.environ["WANDB_API_KEY"] = my_wandb_api_key # my Wandb api key
+            os.environ["WANDB_MODE"] = "dryrun"
+
+        start_time = time.strftime("%H_%M_%S-%d_%m_%Y", time.localtime())
+        pretty_env_name = get_pretty_env_name(args.env_name)
+        experiment_name = f"{args.exp_name}_{pretty_env_name}_{args.seed}_{start_time}"
+            
+        print('_'*50)
+        print('Creating wandboard...')
+        print('_'*50)
+        wandb_save_dir = os.path.join(os.path.abspath(os.getcwd()),f"wandb_{pretty_env_name}")
+        if not os.path.exists(wandb_save_dir):
+            os.makedirs(wandb_save_dir)
+        wandb.init(project='Residual Policy Learning', entity='turbohiro',\
+                   sync_tensorboard=True, config=vars(args), name=experiment_name,\
+                   save_code=True, dir=wandb_save_dir, group=f"{pretty_env_name}")
+        writer = SummaryWriter(f"{wandb.run.dir}/{experiment_name}")
+        weight_save_path = os.path.join(wandb.run.dir, "model.ckpt")
+    ##########################################################################
+    print('_'*50)
+    print('Arguments:')
+    for arg in vars(args):
+        print(f'{arg} = {getattr(args, arg)}')
+    print('_'*50)
+    # initialise the agent
+    trainer = DDPG_Agent(args, env, save_dir=weight_save_path, device=device, writer=writer)
+    # train the agent
+    trainer.train()
+
+    # close env and writer
+    env.close()
+    if writer:
+        writer.close()
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/conda-environment.yaml b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/conda-environment.yaml
new file mode 100644
index 0000000..e8dbed5
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/conda-environment.yaml
@@ -0,0 +1,142 @@
+name: RPL
+channels:
+  - defaults
+dependencies:
+  - _libgcc_mutex=0.1=main
+  - _openmp_mutex=4.5=1_gnu
+  - _pytorch_select=0.1=cpu_0
+  - blas=1.0=mkl
+  - ca-certificates=2022.4.26=h06a4308_0
+  - certifi=2021.10.8=py38h06a4308_2
+  - cffi=1.15.0=py38hd667e15_1
+  - freetype=2.11.0=h70c0345_0
+  - future=0.18.2=py38_1
+  - giflib=5.2.1=h7b6447c_0
+  - intel-openmp=2019.4=243
+  - jpeg=9e=h7f8727e_0
+  - lcms2=2.12=h3be6417_0
+  - ld_impl_linux-64=2.35.1=h7274673_9
+  - libffi=3.3=he6710b0_2
+  - libgcc-ng=9.3.0=h5101ec6_17
+  - libgomp=9.3.0=h5101ec6_17
+  - libmklml=2019.0.5=h06a4308_0
+  - libpng=1.6.37=hbc83047_0
+  - libstdcxx-ng=9.3.0=hd4cf53a_17
+  - libtiff=4.2.0=h85742a9_0
+  - libwebp=1.2.2=h55f646e_0
+  - libwebp-base=1.2.2=h7f8727e_0
+  - lz4-c=1.9.3=h295c915_1
+  - mkl=2020.2=256
+  - mkl-service=2.3.0=py38he904b0f_0
+  - mkl_fft=1.3.0=py38h54f3939_0
+  - mkl_random=1.1.1=py38h0573a6f_0
+  - ncurses=6.3=h7f8727e_2
+  - ninja=1.10.2=h06a4308_5
+  - ninja-base=1.10.2=hd09550d_5
+  - openssl=1.1.1o=h7f8727e_0
+  - pillow=9.0.1=py38h22f2fdc_0
+  - pip=21.2.4=py38h06a4308_0
+  - pycparser=2.21=pyhd3eb1b0_0
+  - python=3.8.13=h12debd9_0
+  - pytorch=1.7.1=cpu_py38h6a09485_0
+  - readline=8.1.2=h7f8727e_1
+  - setuptools=61.2.0=py38h06a4308_0
+  - six=1.16.0=pyhd3eb1b0_1
+  - sqlite=3.38.3=hc218d9a_0
+  - tk=8.6.11=h1ccaba5_1
+  - torchvision=0.8.2=cpu_py38ha229d99_0
+  - typing-extensions=4.1.1=hd3eb1b0_0
+  - typing_extensions=4.1.1=pyh06a4308_0
+  - wheel=0.37.1=pyhd3eb1b0_0
+  - xz=5.2.5=h7f8727e_1
+  - zlib=1.2.12=h7f8727e_2
+  - zstd=1.4.9=haebb681_0
+  - pip:
+    - absl-py==1.0.0
+    - asttokens==2.0.5
+    - attrs==21.4.0
+    - backcall==0.2.0
+    - cachetools==5.0.0
+    - charset-normalizer==2.0.12
+    - click==8.1.3
+    - cloudpickle==2.0.0
+    - cython==0.29.28
+    - decorator==5.1.1
+    - docker-pycreds==0.4.0
+    - executing==0.8.3
+    - fasteners==0.15
+    - free-mujoco-py==2.1.6
+    - gitdb==4.0.9
+    - gitpython==3.1.27
+    - glfw==1.12.0
+    - google-auth==2.6.6
+    - google-auth-oauthlib==0.4.6
+    - grpcio==1.46.1
+    - gym==0.23.1
+    - gym-notices==0.0.6
+    - gym-robotics==0.1.0
+    - h5py==3.6.0
+    - idna==3.3
+    - imagehash==4.2.1
+    - imageio==2.19.1
+    - imageio-ffmpeg==0.4.7
+    - importlib-metadata==4.11.3
+    - iniconfig==1.1.1
+    - ipdb==0.13.9
+    - ipython==8.3.0
+    - jedi==0.18.1
+    - llvmlite==0.36.0
+    - markdown==3.3.7
+    - matplotlib-inline==0.1.3
+    - monotonic==1.6
+    - mpi4py==3.1.3
+    - mujoco-py==2.1.2.14
+    - numba==0.53.1
+    - numpy==1.22.3
+    - oauthlib==3.2.0
+    - openvr==1.16.802
+    - packaging==21.3
+    - parso==0.8.3
+    - pathtools==0.1.2
+    - pexpect==4.8.0
+    - pickleshare==0.7.5
+    - pluggy==1.0.0
+    - promise==2.3
+    - prompt-toolkit==3.0.29
+    - protobuf==3.20.1
+    - psutil==5.9.0
+    - ptyprocess==0.7.0
+    - pure-eval==0.2.2
+    - py==1.11.0
+    - pyasn1==0.4.8
+    - pyasn1-modules==0.2.8
+    - pybullet==3.2.5
+    - pygments==2.12.0
+    - pyopengl==3.1.6
+    - pyparsing==3.0.9
+    - pytest==7.1.2
+    - python-dateutil==2.8.2
+    - pywavelets==1.3.0
+    - pyyaml==6.0
+    - requests==2.27.1
+    - requests-oauthlib==1.3.1
+    - rsa==4.8
+    - scipy==1.8.0
+    - sentry-sdk==1.5.12
+    - setproctitle==1.2.3
+    - shortuuid==1.0.9
+    - smmap==5.0.0
+    - stack-data==0.2.0
+    - tensorboard==2.9.0
+    - tensorboard-data-server==0.6.1
+    - tensorboard-plugin-wit==1.8.1
+    - toml==0.10.2
+    - tomli==2.0.1
+    - tqdm==4.64.0
+    - traitlets==5.2.0
+    - urllib3==1.26.9
+    - wandb==0.12.16
+    - wcwidth==0.2.5
+    - werkzeug==2.1.2
+    - zipp==3.8.0
+prefix: /homeL/wchen/anaconda3/envs/RPL
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/config.yaml b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/config.yaml
new file mode 100644
index 0000000..68b98ea
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/config.yaml
@@ -0,0 +1,132 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.16
+    code_path: code/Residual-Policy-Learning/RL/ddpg/ddpg.py
+    framework: torch
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.13
+    start_time: 1654606483
+    t:
+      1:
+      - 1
+      - 55
+      3:
+      - 13
+      - 16
+      - 35
+      4: 3.8.13
+      5: 0.12.16
+      8:
+      - 5
+action_l2:
+  desc: null
+  value: 1.0
+activation:
+  desc: null
+  value: relu
+actor_lr:
+  desc: null
+  value: 0.001
+actor_optim:
+  desc: null
+  value: adam
+batch_size:
+  desc: null
+  value: 256
+beta:
+  desc: null
+  value: 1.0
+beta_monitor:
+  desc: null
+  value: critic
+buffer_size:
+  desc: null
+  value: 1000000
+clip_obs:
+  desc: null
+  value: 200
+clip_range:
+  desc: null
+  value: 5
+coin_flipping_prob:
+  desc: null
+  value: 0.5
+critic_lr:
+  desc: null
+  value: 0.001
+critic_optim:
+  desc: null
+  value: adam
+device:
+  desc: null
+  value: cpu
+dryrun:
+  desc: null
+  value: false
+env_name:
+  desc: null
+  value: NutAssembly
+exp_name:
+  desc: null
+  value: rl
+gamma:
+  desc: null
+  value: 0.98
+hidden_dims:
+  desc: null
+  value:
+  - 256
+  - 256
+  - 256
+loss_fn:
+  desc: null
+  value: mse
+max_grad_norm:
+  desc: null
+  value: 0.5
+mpi:
+  desc: null
+  value: false
+n_batches:
+  desc: null
+  value: 40
+n_cycles:
+  desc: null
+  value: 50
+n_epochs:
+  desc: null
+  value: 500
+n_test_rollouts:
+  desc: null
+  value: 50
+noise_eps:
+  desc: null
+  value: 0.2
+num_rollouts_per_mpi:
+  desc: null
+  value: 2
+polyak:
+  desc: null
+  value: 0.95
+random_eps:
+  desc: null
+  value: 0.3
+replay_k:
+  desc: null
+  value: 4
+replay_strategy:
+  desc: null
+  value: future
+seed:
+  desc: null
+  value: 1
+torch_deterministic:
+  desc: null
+  value: true
+weight_decay:
+  desc: null
+  value: 0.0
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/diff.patch b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/diff.patch
new file mode 100644
index 0000000..de4265e
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/diff.patch
@@ -0,0 +1,25 @@
+diff --git a/requirements.txt b/requirements.txt
+index 700015d..d6e1198 100644
+--- a/requirements.txt
++++ b/requirements.txt
+@@ -1,3 +1 @@
+-cffi==1.14.0
+-PyOpenGL==3.1.5
+-pytest==6.1.2
++-e .
+diff --git a/setup.py b/setup.py
+index 109baa6..f1e4f2c 100644
+--- a/setup.py
++++ b/setup.py
+@@ -23,4 +23,11 @@ setup(
+     eager_resources=["*"],
+     include_package_data=True,
+     python_requires=">=3",
++    description="robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
++    author="Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martín-Martín",
++    url="https://github.com/ARISE-Initiative/robosuite",
++    author_email="yukez@cs.utexas.edu",
++    version="1.3.2",
++    long_description=long_description,
++    long_description_content_type="text/markdown",
+ )
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/events.out.tfevents.1654606486.tamsGPU2.32128.0 b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/events.out.tfevents.1654606486.tamsGPU2.32128.0
new file mode 120000
index 0000000..7e89c24
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/events.out.tfevents.1654606486.tamsGPU2.32128.0
@@ -0,0 +1 @@
+/homeL/wchen/Assembly_RPL/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/rl_NutAssembly_1_14_54_42-07_06_2022/events.out.tfevents.1654606486.tamsGPU2.32128.0
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/requirements.txt b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/requirements.txt
new file mode 100644
index 0000000..6bdd895
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/requirements.txt
@@ -0,0 +1,103 @@
+absl-py==1.0.0
+asttokens==2.0.5
+attrs==21.4.0
+backcall==0.2.0
+cachetools==5.0.0
+certifi==2021.10.8
+cffi==1.15.0
+charset-normalizer==2.0.12
+click==8.1.3
+cloudpickle==2.0.0
+cython==0.29.28
+decorator==5.1.1
+docker-pycreds==0.4.0
+executing==0.8.3
+fasteners==0.15
+free-mujoco-py==2.1.6
+future==0.18.2
+gitdb==4.0.9
+gitpython==3.1.27
+glfw==1.12.0
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.46.1
+gym-notices==0.0.6
+gym-robotics==0.1.0
+gym==0.23.1
+h5py==3.6.0
+idna==3.3
+imagehash==4.2.1
+imageio-ffmpeg==0.4.7
+imageio==2.19.1
+importlib-metadata==4.11.3
+iniconfig==1.1.1
+ipdb==0.13.9
+ipython==8.3.0
+jedi==0.18.1
+llvmlite==0.36.0
+markdown==3.3.7
+matplotlib-inline==0.1.3
+mkl-fft==1.3.0
+mkl-random==1.1.1
+mkl-service==2.3.0
+monotonic==1.6
+mpi4py==3.1.3
+mujoco-py==2.1.2.14
+numba==0.53.1
+numpy==1.22.3
+oauthlib==3.2.0
+openvr==1.16.802
+packaging==21.3
+parso==0.8.3
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==9.0.1
+pip==21.2.4
+pluggy==1.0.0
+promise==2.3
+prompt-toolkit==3.0.29
+protobuf==3.20.1
+psutil==5.9.0
+ptyprocess==0.7.0
+pure-eval==0.2.2
+py==1.11.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybullet==3.2.5
+pycparser==2.21
+pygments==2.12.0
+pyopengl==3.1.6
+pyparsing==3.0.9
+pytest==7.1.2
+python-dateutil==2.8.2
+pywavelets==1.3.0
+pyyaml==6.0
+requests-oauthlib==1.3.1
+requests==2.27.1
+robosuite==1.3.2
+rsa==4.8
+scipy==1.8.0
+sentry-sdk==1.5.12
+setproctitle==1.2.3
+setuptools==61.2.0
+shortuuid==1.0.9
+six==1.16.0
+smmap==5.0.0
+stack-data==0.2.0
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.9.0
+toml==0.10.2
+tomli==2.0.1
+torch==1.7.1
+torchvision==0.8.0a0
+tqdm==4.64.0
+traitlets==5.2.0
+typing-extensions==4.1.1
+urllib3==1.26.9
+wandb==0.12.16
+wcwidth==0.2.5
+werkzeug==2.1.2
+wheel==0.37.1
+zipp==3.8.0
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/rl_NutAssembly_1_14_54_42-07_06_2022/events.out.tfevents.1654606486.tamsGPU2.32128.0 b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/rl_NutAssembly_1_14_54_42-07_06_2022/events.out.tfevents.1654606486.tamsGPU2.32128.0
new file mode 100644
index 0000000..c3bfd8e
Binary files /dev/null and b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/rl_NutAssembly_1_14_54_42-07_06_2022/events.out.tfevents.1654606486.tamsGPU2.32128.0 differ
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/wandb-metadata.json b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/wandb-metadata.json
new file mode 100644
index 0000000..a74dd1d
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/wandb-metadata.json
@@ -0,0 +1,33 @@
+{
+    "os": "Linux-5.4.0-100-generic-x86_64-with-glibc2.17",
+    "python": "3.8.13",
+    "heartbeatAt": "2022-06-07T12:54:44.208783",
+    "startedAt": "2022-06-07T12:54:43.403912",
+    "docker": null,
+    "gpu": "NVIDIA GeForce RTX 2080 Ti",
+    "gpu_count": 2,
+    "cpu_count": 24,
+    "cuda": null,
+    "args": [
+        "--env_name",
+        "NutAssembly",
+        "--seed",
+        "1",
+        "--n_epochs",
+        "500",
+        "--exp_name",
+        "rl"
+    ],
+    "state": "running",
+    "program": "RL/ddpg/ddpg.py",
+    "codePath": "Residual-Policy-Learning/RL/ddpg/ddpg.py",
+    "git": {
+        "remote": "https://github.com/turbohiro/Assembly_RPL",
+        "commit": "57769111097c70fc522f3c051ef234a5d053c7f6"
+    },
+    "email": null,
+    "root": "/homeL/wchen/Assembly_RPL",
+    "host": "tamsGPU2",
+    "username": "wchen",
+    "executable": "/homeL/wchen/anaconda3/envs/RPL/bin/python"
+}
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/wandb-summary.json b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/wandb-summary.json
new file mode 100644
index 0000000..e682bae
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/files/wandb-summary.json
@@ -0,0 +1 @@
+{"_wandb": {"runtime": 5}}
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/run-3st01aze.wandb b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/run-3st01aze.wandb
new file mode 100644
index 0000000..7432348
Binary files /dev/null and b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145443-3st01aze/run-3st01aze.wandb differ
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py
new file mode 100644
index 0000000..33b7b76
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py
@@ -0,0 +1,504 @@
+"""
+    DDPG with HER
+"""
+import copy
+import gym
+import argparse
+import torch
+from torch import nn
+from torch import optim
+import os
+from datetime import datetime
+import numpy as np
+from typing import Tuple
+
+from RL.ddpg.models import actor, critic
+from RL.ddpg.replay_buffer import replay_buffer
+from her.her import her_sampler
+
+OPTIMIZERS = {
+    'adam': optim.Adam,
+    'adamax': optim.Adamax,
+    'rmsprop': optim.RMSprop,
+}
+
+LOSS_FN = {
+    'mse': nn.MSELoss(),
+    'smooth_l1': nn.SmoothL1Loss(),
+    'l1': nn.L1Loss()
+}
+
+class DDPG_Agent:
+    def __init__(self, args:argparse.Namespace, env, save_dir: str, device:str, writer=None):
+        """
+            Module for the DDPG agent along with HER
+            Parameters:
+            -----------
+            args: argparse.Namespace
+                args should contain the following:
+            env: gym.Env
+                OpenAI type gym environment
+            save_dir: str
+                Path to save the network weights, checkpoints
+            device: str
+                device to run the training process on
+                Choices: 'cpu', 'cuda'
+            writer: tensorboardX
+                tensorboardX to log metrics like losses, rewards, success_rates, etc.
+        """
+        self.args = args
+        self.env = env
+        self.env_params = self.get_env_params(env)
+        self.save_dir = save_dir
+        self.device = device
+        self.writer = writer
+        self.sim_steps = 0      # a count to keep track of number of simulation steps
+        self.train_steps = 0    # a count to keep track of number of training steps
+
+        # create the network
+        self.actor_network = actor(args, self.env_params).to(device)
+        self.critic_network = critic(args, self.env_params).to(device)
+        # build up the target network
+        self.actor_target_network = actor(args, self.env_params).to(device)
+        self.critic_target_network = critic(args, self.env_params).to(device)
+
+        # load the weights into the target networks
+        self.actor_target_network.load_state_dict(self.actor_network.state_dict())
+        self.critic_target_network.load_state_dict(self.critic_network.state_dict())
+
+        # create the optimizer
+        self.actor_optim  = OPTIMIZERS[args.actor_optim](self.actor_network.parameters(),\
+                                                         lr=self.args.actor_lr,\
+                                                         weight_decay=self.args.weight_decay)
+        self.critic_optim = OPTIMIZERS[args.critic_optim](self.critic_network.parameters(),
+                                                         lr=self.args.critic_lr,\
+                                                         weight_decay=self.args.weight_decay)
+        self.burn_in_done = False   # to check if burn-in is done or not
+
+        # loss function for DDPG
+        self.criterion = LOSS_FN[args.loss_fn]
+
+        # her sampler
+        self.her_module = her_sampler(replay_strategy = self.args.replay_strategy, \
+                                      replay_k = self.args.replay_k, \
+                                      reward_func = self.env.compute_reward)
+        # create the replay buffer
+        self.buffer = replay_buffer(self.env_params, \
+                                    self.args.buffer_size, \
+                                    self.her_module.sample_her_transitions)
+    
+    def get_env_params(self, env):
+        """
+            Get the environment parameters
+        """
+        obs = env.reset()
+        # close the environment
+        params = {'obs': obs['observation'].shape[0],
+                'goal': obs['desired_goal'].shape[0],
+                'action': env.action_space.shape[0],
+                'action_max': env.action_space.high[0],
+                }
+        try:
+            params['max_timesteps'] = env._max_episode_steps
+        # for custom envs
+        except:
+            params['max_timesteps'] = env.max_episode_steps
+        return params
+    
+    def set_actor_lr(self, loss:float, prev_loss:float ,verbose:bool=True):
+        """
+            Set the learning rate of the actor network
+            to either the original learning rate
+            or zero depending on the burn-in parameter
+            diff_loss = |loss - prev_loss|
+            if rl:
+                actor_lr = original_actor_lr
+            if residual learning and diff_loss < beta:
+                actor_lr = original_actor_lr
+            elif residual learning and diff_loss > beta:
+                actor_lr = 0
+            Parameters:
+            -----------
+            loss: float
+                Mean of the losses till the current epoch
+            prev_loss: float
+                Mean of losses till the last epoch
+            verbose: bool
+                To print the change in learning rate
+        """
+        lr = self.args.actor_lr
+        coin_flipping = False
+        # loss is zero only in the first epoch hence do not change lr then
+        # and that's why give a large value to diff_loss
+        diff_loss = 100 if loss == 0 else abs(loss - prev_loss)
+        if not self.burn_in_done:
+            if self.args.exp_name == 'res' and diff_loss > self.args.beta:
+                lr = 0.0
+                coin_flipping = True
+            elif self.args.exp_name == 'res' and diff_loss <= self.args.beta:
+                if verbose:
+                    print('_'*80)
+                    print(f'Burn-in of the critic done. Changing actor_lr from 0.0 to {self.args.actor_lr}')
+                    print('_'*80)
+                self.burn_in_done = True
+
+        for param_group in self.actor_optim.param_groups:
+            param_group['lr'] = lr
+        return coin_flipping
+        
+    def train(self):
+        """
+            Run the episodes for training
+        """
+        print('_'*50)
+        print('Beginning the training...')
+        print('_'*50)
+        num_cycles = 0
+        actor_losses = [0.0]    # to store actor losses for burn-in
+        critic_losses = [0.0]   # to store critic losses for burn-in
+        prev_losses = [0.0]
+        coin_flipping = False   # whether the whole episode should be noise and randomness free
+        deterministic = False   # choose whether we want deterministic or not  
+        for epoch in range(self.args.n_epochs):
+
+            # change the actor learning rate from zero to actor_lr by checking burn-in
+            # check config.py for more information on args.beta_monitor
+            if self.args.beta_monitor == 'actor':
+                coin_flipping = self.set_actor_lr(np.mean(actor_losses), np.mean(prev_losses))
+                prev_losses = actor_losses.copy()
+            elif self.args.beta_monitor == 'critic':
+                coin_flipping = self.set_actor_lr(np.mean(critic_losses), np.mean(prev_losses))
+                prev_losses = critic_losses.copy()
+
+            for cycle in range(self.args.n_cycles):
+
+                ep_obs, ep_ag, ep_g, ep_actions = [], [], [], []
+                # reset the environment
+                observation = self.env.reset()
+                observation_new = copy.deepcopy(observation)
+                obs = observation['observation']
+                ag = observation['achieved_goal']
+                g = observation['desired_goal']
+
+                random_eps = self.args.random_eps
+                noise_eps  = self.args.noise_eps
+                if coin_flipping:
+                    deterministic = np.random.random() < self.args.coin_flipping_prob  # NOTE/TODO change here
+                if deterministic:
+                    random_eps = 0.0
+                    noise_eps = 0.0
+
+                for t in range(self.env_params['max_timesteps']):
+                    # take actions 
+                    with torch.no_grad():
+                        state = self.preprocess_inputs(obs, g)
+                        pi = self.actor_network(state)
+                        if self.args.exp_name == 'res':
+                            controller_action = self.get_controller_actions(observation_new)
+                            action = self.select_actions(pi, noise_eps=noise_eps, random_eps= random_eps, controller_action=controller_action)
+                        else:
+                            action = self.select_actions(pi, noise_eps=noise_eps, random_eps= random_eps, controller_action=None)
+                    # give the action to the environment
+                    observation_new, _, _, info = self.env.step(action)
+                    self.sim_steps += 1                     # increase the simulation timestep by one
+                    obs_new = observation_new['observation']
+                    ag_new = observation_new['achieved_goal']
+                    # append rollouts
+                    ep_obs.append(obs.copy())
+                    ep_ag.append(ag.copy())
+                    ep_g.append(g.copy())
+                    ep_actions.append(action.copy())
+                    # re-assign the observation
+                    obs = obs_new
+                    ag = ag_new
+                # append last states in the array, extend the episode chain (state machine)
+                ep_obs.append(obs.copy())
+                ep_ag.append(ag.copy())
+                # convert to np arrays
+                ep_obs = np.expand_dims(np.array(ep_obs),0)
+                ep_ag = np.expand_dims(np.array(ep_ag),0)
+                ep_g = np.expand_dims(np.array(ep_g),0)
+                ep_actions = np.expand_dims(np.array(ep_actions),0)
+
+                # store them in buffer
+                self.buffer.store_episode([ep_obs, ep_ag, ep_g, ep_actions])
+
+                actor_loss_cycle = 0; critic_loss_cycle = 0 
+                for batch in range(self.args.n_batches):
+                    # train the network with 'n_batches' number of batches
+                    actor_loss, critic_loss = self.update_network(self.train_steps)
+                    self.train_steps += 1
+                    actor_losses.append(actor_loss)
+                    critic_losses.append(critic_loss)
+                    actor_loss_cycle += actor_loss
+                    critic_loss_cycle += critic_loss
+                if self.writer:
+                    self.writer.add_scalar('Cycle Losses/Actor Loss', actor_loss_cycle, num_cycles)
+                    self.writer.add_scalar('Cycle Losses/Critic Loss', critic_loss_cycle, num_cycles)
+                self.polyak_update_networks(self.actor_target_network, self.actor_network)
+                self.polyak_update_networks(self.critic_target_network, self.critic_network)
+                num_cycles += 1
+            # evaluate the agent
+            success_rate = self.eval_agent()
+            print(f'Epoch Critic: {np.mean(critic_losses):.3f} Epoch Actor:{np.mean(actor_losses):.3f}')
+            print(f'Epoch:{epoch}\tSuccess Rate:{success_rate:.3f}')
+            if self.writer:
+                self.writer.add_scalar('Success Rate/Success Rate', success_rate, self.sim_steps)
+                self.writer.add_scalar('Epoch Losses/Average Critic Loss', np.mean(critic_losses), epoch)
+                self.writer.add_scalar('Epoch Losses/Average Actor Loss', np.mean(actor_losses), epoch)
+            
+            self.save_checkpoint(self.save_dir)
+
+    def preprocess_inputs(self, obs:np.ndarray, g:np.ndarray) -> torch.Tensor:
+        """
+            Concatenate state and goal
+            and convert them to torch tensors
+            and then transfer them to either CPU of GPU
+        """
+        # concatenate the stuffs
+        inputs = np.concatenate([obs, g])   #observation input (state) and goal label (ground truth)
+        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)
+        inputs = inputs.to(self.device)
+        return inputs
+    
+    def get_controller_actions(self, obs:dict):
+        """
+            Return the controller action if residual learning
+        """
+        return self.env.controller_action(obs, take_action=False)
+
+    def select_actions(self, pi: torch.Tensor, noise_eps:float, random_eps:float, controller_action=None) -> np.ndarray:
+        """
+            Take action
+            with a probability of self.args.random_eps, it will take random actions
+            otherwise, this will add a gaussian noise to the action along with clipping
+            pi: torch.Tensor
+                The action given by the actor
+            noise_eps: float
+                The random gaussian noise added to actor output
+            random_eps: float
+                Probability of taking random action
+            controller_action: None or np.ndarray
+                To subtract from random action
+        """
+        # transfer action from CUDA to CPU if using GPU and make numpy array out of it
+        action = pi.cpu().numpy().squeeze()
+        # add the gaussian
+        action += noise_eps * self.env_params['action_max'] * np.random.randn(*action.shape)
+        action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])  #make action values are limited a feasible range
+
+        # random actions
+        random_actions = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \
+                                            size=self.env_params['action'])
+        # if residual learning, subtract the controller action so that we don't add it twice
+        if self.args.exp_name == 'res':
+            random_actions = random_actions - controller_action
+        # choose whether to take random actions or not
+        rand = np.random.binomial(1, random_eps, 1)[0]
+        action += rand * (random_actions - action)  # will be equal to either random_actions or action
+        return action
+
+    def preprocess_og(self, o:np.ndarray, g:np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
+        """
+            Perform observation clipping
+        """
+        o = np.clip(o, -self.args.clip_obs, self.args.clip_obs)
+        g = np.clip(g, -self.args.clip_obs, self.args.clip_obs)
+        return o, g
+
+    def polyak_update_networks(self, target, source):
+        """
+            Polyak averaging of target and main networks; Also known as soft update of networks
+            target_net_params = (1 - polyak) * main_net_params + polyak * target_net_params
+        """
+        for target_param, param in zip(target.parameters(), source.parameters()):
+            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)
+
+    def update_network(self, step:int) -> Tuple[float, float]:
+        """
+            The actual DDPG training
+        """
+
+        # sample the episodes
+        transitions = self.buffer.sample(self.args.batch_size)
+
+        # pre-process the observation and goal
+        o, o_next, g = transitions['obs'], transitions['obs_next'], transitions['g']
+        transitions['obs'], transitions['g'] = self.preprocess_og(o, g)
+        transitions['obs_next'], transitions['g_next'] = self.preprocess_og(o_next, g)
+
+        # concatenate obs and goal
+        states = np.concatenate([transitions['obs'], transitions['g']], axis=1)
+        next_states = np.concatenate([transitions['obs_next'], transitions['g_next']], axis=1)
+
+        # convert to tensor
+        states = torch.tensor(states, dtype=torch.float32).to(self.device)
+        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)
+        actions = torch.tensor(transitions['actions'], dtype=torch.float32).to(self.device)
+        rewards = torch.tensor(transitions['r'], dtype=torch.float32).to(self.device)
+
+        with torch.no_grad():
+            actions_next = self.actor_target_network(next_states)
+            q_next_value = self.critic_target_network(next_states, actions_next)
+            q_next_value = q_next_value.detach()
+            target_q_value = rewards + self.args.gamma * q_next_value
+            target_q_value = target_q_value.detach()
+            # clip the returns
+            clip_return = 1 / (1-self.args.gamma)
+            target_q_value = torch.clamp(target_q_value, -clip_return, 0)
+        
+        # critic loss
+        q_value = self.critic_network(states, actions)
+        critic_loss = self.criterion(target_q_value, q_value)   # loss (mostly MSE)
+
+        # actor loss
+        actions_pred = self.actor_network(states)
+        actor_loss = -self.critic_network(states, actions_pred).mean()   #the max of q values, the better of actor(policy)
+        actor_loss = actor_loss + self.args.action_l2 * (actions_pred / self.env_params['action_max']).pow(2).mean()
+
+        # backpropagate
+        self.actor_optim.zero_grad()    # zero the gradients
+        actor_loss.backward()           # backward prop
+        self.actor_optim.step()         # take step towards gradient direction
+
+        self.critic_optim.zero_grad()    # zero the gradients
+        critic_loss.backward()           # backward prop
+        self.critic_optim.step()         # take step towards gradient directions
+
+        return actor_loss.item(), critic_loss.item()
+        
+    def eval_agent(self) -> float:
+        """
+            Evaluate the agent using the trained policy
+            performs n_test_rollouts in the environment
+            and returns
+        """
+        successes = []
+        for _ in range(self.args.n_test_rollouts):
+            success = np.zeros(self.env_params['max_timesteps'])
+            observation = self.env.reset()
+            obs = observation['observation']
+            g = observation['desired_goal']
+            for i in range(self.env_params['max_timesteps']):
+                with torch.no_grad():
+                    input_tensor = self.preprocess_inputs(obs,g)
+                    pi = self.actor_network(input_tensor)
+                    actions = pi.detach().cpu().numpy().squeeze()
+                observation_new, _, _, info = self.env.step(actions)
+                obs = observation_new['observation']
+                g = observation_new['desired_goal']
+                success[i] = info['is_success']
+            successes.append(success)
+        successes = np.array(successes)
+        return np.mean(successes[:,-1]) # return mean of only final steps success
+
+    def save_checkpoint(self, path:str):
+        """
+            Saves the model in the wandb experiment run directory
+            This will store the 
+                * model state_dict
+                * optimizer state_dict
+                * args/hparams
+            param:
+                path: str
+                    path to the wandb run directory
+                    Example: os.path.join(wandb.run.dir, "model.ckpt")
+        """
+        checkpoint = {}
+        checkpoint['args'] = vars(self.args)
+        checkpoint['actor_state_dict'] = self.actor_network.state_dict()
+        checkpoint['env_name'] = self.args.env_name
+        torch.save(checkpoint, path)
+
+    def load_checkpoint(self, path:str):
+        """
+            Load the trained model weights
+            param:
+                path: str
+                    path to the saved weights file
+        """
+        use_cuda = torch.cuda.is_available()
+        device   = torch.device("cuda" if use_cuda else "cpu")
+        checkpoint_dict = torch.load(path, map_location=device)
+        self.actor_network.load_state_dict(checkpoint_dict['actor_state_dict'])
+
+if __name__ == "__main__":
+    # run from main folder as `python RL/ddpg.py`
+    import time
+    import wandb
+    from torch.utils.tensorboard import SummaryWriter
+
+    from ddpg_config import args
+    from utils import connected_to_internet, make_env, get_pretty_env_name
+    import pdb
+
+    # check whether GPU is available or not
+    use_cuda = torch.cuda.is_available()
+    device = torch.device("cuda" if use_cuda else "cpu")
+    print('_'*50)
+    print('Device:',device)
+    print('_'*50)
+    args.device = device
+    args.mpi = False        # not running on mpi mode
+    #####################################
+
+    env = make_env(args.env_name)   # initialise the environment
+    #env = gym.make(args.env_name)   # initialise the environment
+    
+    # set the random seeds for everything
+    # random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+    env.seed(args.seed)
+    env.action_space.seed(args.seed)
+    env.observation_space.seed(args.seed)
+    #####################################
+
+    # book keeping to log stuff
+    if args.dryrun:
+        writer = None
+        weight_save_path = 'model_dryrun.ckpt'
+    else:
+        # check internet connection
+        # for offline wandb. Will load everything on cloud afterwards
+        if not connected_to_internet():
+            import json
+            # save a json file with your wandb api key in your home folder as {'my_wandb_api_key': 'INSERT API HERE'}
+            # NOTE this is only for running on MIT Supercloud
+            with open(os.path.expanduser('~')+'/keys.json') as json_file: 
+                key = json.load(json_file)
+                my_wandb_api_key = key['my_wandb_api_key'] # NOTE change here as well
+            os.environ["WANDB_API_KEY"] = my_wandb_api_key # my Wandb api key
+            os.environ["WANDB_MODE"] = "dryrun"
+
+        start_time = time.strftime("%H_%M_%S-%d_%m_%Y", time.localtime())
+        pretty_env_name = get_pretty_env_name(args.env_name)
+        experiment_name = f"{args.exp_name}_{pretty_env_name}_{args.seed}_{start_time}"
+            
+        print('_'*50)
+        print('Creating wandboard...')
+        print('_'*50)
+        wandb_save_dir = os.path.join(os.path.abspath(os.getcwd()),f"wandb_{pretty_env_name}")
+        if not os.path.exists(wandb_save_dir):
+            os.makedirs(wandb_save_dir)
+        wandb.init(project='Residual Policy Learning', entity='turbohiro',\
+                   sync_tensorboard=True, config=vars(args), name=experiment_name,\
+                   save_code=True, dir=wandb_save_dir, group=f"{pretty_env_name}")
+        writer = SummaryWriter(f"{wandb.run.dir}/{experiment_name}")
+        weight_save_path = os.path.join(wandb.run.dir, "model.ckpt")
+    ##########################################################################
+    print('_'*50)
+    print('Arguments:')
+    for arg in vars(args):
+        print(f'{arg} = {getattr(args, arg)}')
+    print('_'*50)
+    # initialise the agent
+    trainer = DDPG_Agent(args, env, save_dir=weight_save_path, device=device, writer=writer)
+    # train the agent
+    trainer.train()
+
+    # close env and writer
+    env.close()
+    if writer:
+        writer.close()
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/conda-environment.yaml b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/conda-environment.yaml
new file mode 100644
index 0000000..e8dbed5
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/conda-environment.yaml
@@ -0,0 +1,142 @@
+name: RPL
+channels:
+  - defaults
+dependencies:
+  - _libgcc_mutex=0.1=main
+  - _openmp_mutex=4.5=1_gnu
+  - _pytorch_select=0.1=cpu_0
+  - blas=1.0=mkl
+  - ca-certificates=2022.4.26=h06a4308_0
+  - certifi=2021.10.8=py38h06a4308_2
+  - cffi=1.15.0=py38hd667e15_1
+  - freetype=2.11.0=h70c0345_0
+  - future=0.18.2=py38_1
+  - giflib=5.2.1=h7b6447c_0
+  - intel-openmp=2019.4=243
+  - jpeg=9e=h7f8727e_0
+  - lcms2=2.12=h3be6417_0
+  - ld_impl_linux-64=2.35.1=h7274673_9
+  - libffi=3.3=he6710b0_2
+  - libgcc-ng=9.3.0=h5101ec6_17
+  - libgomp=9.3.0=h5101ec6_17
+  - libmklml=2019.0.5=h06a4308_0
+  - libpng=1.6.37=hbc83047_0
+  - libstdcxx-ng=9.3.0=hd4cf53a_17
+  - libtiff=4.2.0=h85742a9_0
+  - libwebp=1.2.2=h55f646e_0
+  - libwebp-base=1.2.2=h7f8727e_0
+  - lz4-c=1.9.3=h295c915_1
+  - mkl=2020.2=256
+  - mkl-service=2.3.0=py38he904b0f_0
+  - mkl_fft=1.3.0=py38h54f3939_0
+  - mkl_random=1.1.1=py38h0573a6f_0
+  - ncurses=6.3=h7f8727e_2
+  - ninja=1.10.2=h06a4308_5
+  - ninja-base=1.10.2=hd09550d_5
+  - openssl=1.1.1o=h7f8727e_0
+  - pillow=9.0.1=py38h22f2fdc_0
+  - pip=21.2.4=py38h06a4308_0
+  - pycparser=2.21=pyhd3eb1b0_0
+  - python=3.8.13=h12debd9_0
+  - pytorch=1.7.1=cpu_py38h6a09485_0
+  - readline=8.1.2=h7f8727e_1
+  - setuptools=61.2.0=py38h06a4308_0
+  - six=1.16.0=pyhd3eb1b0_1
+  - sqlite=3.38.3=hc218d9a_0
+  - tk=8.6.11=h1ccaba5_1
+  - torchvision=0.8.2=cpu_py38ha229d99_0
+  - typing-extensions=4.1.1=hd3eb1b0_0
+  - typing_extensions=4.1.1=pyh06a4308_0
+  - wheel=0.37.1=pyhd3eb1b0_0
+  - xz=5.2.5=h7f8727e_1
+  - zlib=1.2.12=h7f8727e_2
+  - zstd=1.4.9=haebb681_0
+  - pip:
+    - absl-py==1.0.0
+    - asttokens==2.0.5
+    - attrs==21.4.0
+    - backcall==0.2.0
+    - cachetools==5.0.0
+    - charset-normalizer==2.0.12
+    - click==8.1.3
+    - cloudpickle==2.0.0
+    - cython==0.29.28
+    - decorator==5.1.1
+    - docker-pycreds==0.4.0
+    - executing==0.8.3
+    - fasteners==0.15
+    - free-mujoco-py==2.1.6
+    - gitdb==4.0.9
+    - gitpython==3.1.27
+    - glfw==1.12.0
+    - google-auth==2.6.6
+    - google-auth-oauthlib==0.4.6
+    - grpcio==1.46.1
+    - gym==0.23.1
+    - gym-notices==0.0.6
+    - gym-robotics==0.1.0
+    - h5py==3.6.0
+    - idna==3.3
+    - imagehash==4.2.1
+    - imageio==2.19.1
+    - imageio-ffmpeg==0.4.7
+    - importlib-metadata==4.11.3
+    - iniconfig==1.1.1
+    - ipdb==0.13.9
+    - ipython==8.3.0
+    - jedi==0.18.1
+    - llvmlite==0.36.0
+    - markdown==3.3.7
+    - matplotlib-inline==0.1.3
+    - monotonic==1.6
+    - mpi4py==3.1.3
+    - mujoco-py==2.1.2.14
+    - numba==0.53.1
+    - numpy==1.22.3
+    - oauthlib==3.2.0
+    - openvr==1.16.802
+    - packaging==21.3
+    - parso==0.8.3
+    - pathtools==0.1.2
+    - pexpect==4.8.0
+    - pickleshare==0.7.5
+    - pluggy==1.0.0
+    - promise==2.3
+    - prompt-toolkit==3.0.29
+    - protobuf==3.20.1
+    - psutil==5.9.0
+    - ptyprocess==0.7.0
+    - pure-eval==0.2.2
+    - py==1.11.0
+    - pyasn1==0.4.8
+    - pyasn1-modules==0.2.8
+    - pybullet==3.2.5
+    - pygments==2.12.0
+    - pyopengl==3.1.6
+    - pyparsing==3.0.9
+    - pytest==7.1.2
+    - python-dateutil==2.8.2
+    - pywavelets==1.3.0
+    - pyyaml==6.0
+    - requests==2.27.1
+    - requests-oauthlib==1.3.1
+    - rsa==4.8
+    - scipy==1.8.0
+    - sentry-sdk==1.5.12
+    - setproctitle==1.2.3
+    - shortuuid==1.0.9
+    - smmap==5.0.0
+    - stack-data==0.2.0
+    - tensorboard==2.9.0
+    - tensorboard-data-server==0.6.1
+    - tensorboard-plugin-wit==1.8.1
+    - toml==0.10.2
+    - tomli==2.0.1
+    - tqdm==4.64.0
+    - traitlets==5.2.0
+    - urllib3==1.26.9
+    - wandb==0.12.16
+    - wcwidth==0.2.5
+    - werkzeug==2.1.2
+    - zipp==3.8.0
+prefix: /homeL/wchen/anaconda3/envs/RPL
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/config.yaml b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/config.yaml
new file mode 100644
index 0000000..310fd27
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/config.yaml
@@ -0,0 +1,132 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.16
+    code_path: code/Residual-Policy-Learning/RL/ddpg/ddpg.py
+    framework: torch
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.13
+    start_time: 1654606561
+    t:
+      1:
+      - 1
+      - 55
+      3:
+      - 13
+      - 16
+      - 35
+      4: 3.8.13
+      5: 0.12.16
+      8:
+      - 5
+action_l2:
+  desc: null
+  value: 1.0
+activation:
+  desc: null
+  value: relu
+actor_lr:
+  desc: null
+  value: 0.001
+actor_optim:
+  desc: null
+  value: adam
+batch_size:
+  desc: null
+  value: 256
+beta:
+  desc: null
+  value: 1.0
+beta_monitor:
+  desc: null
+  value: critic
+buffer_size:
+  desc: null
+  value: 1000000
+clip_obs:
+  desc: null
+  value: 200
+clip_range:
+  desc: null
+  value: 5
+coin_flipping_prob:
+  desc: null
+  value: 0.5
+critic_lr:
+  desc: null
+  value: 0.001
+critic_optim:
+  desc: null
+  value: adam
+device:
+  desc: null
+  value: cpu
+dryrun:
+  desc: null
+  value: false
+env_name:
+  desc: null
+  value: NutAssembly
+exp_name:
+  desc: null
+  value: rl
+gamma:
+  desc: null
+  value: 0.98
+hidden_dims:
+  desc: null
+  value:
+  - 256
+  - 256
+  - 256
+loss_fn:
+  desc: null
+  value: mse
+max_grad_norm:
+  desc: null
+  value: 0.5
+mpi:
+  desc: null
+  value: false
+n_batches:
+  desc: null
+  value: 40
+n_cycles:
+  desc: null
+  value: 50
+n_epochs:
+  desc: null
+  value: 500
+n_test_rollouts:
+  desc: null
+  value: 50
+noise_eps:
+  desc: null
+  value: 0.2
+num_rollouts_per_mpi:
+  desc: null
+  value: 2
+polyak:
+  desc: null
+  value: 0.95
+random_eps:
+  desc: null
+  value: 0.3
+replay_k:
+  desc: null
+  value: 4
+replay_strategy:
+  desc: null
+  value: future
+seed:
+  desc: null
+  value: 1
+torch_deterministic:
+  desc: null
+  value: true
+weight_decay:
+  desc: null
+  value: 0.0
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/diff.patch b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/diff.patch
new file mode 100644
index 0000000..516d4a9
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/diff.patch
@@ -0,0 +1,47 @@
+diff --git a/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py b/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
+index 8d63c3f..0e95116 100644
+--- a/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
++++ b/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
+@@ -57,7 +57,7 @@ class NutAssembly(gym.Env):
+         ob, reward, done, info = self.env.step(action)
+         ob = self.env.observation_spec()
+         observation = {}
+-        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut0_pos'], ob['RoundNut0_quat']))
++        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
+         observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
+         observation['achieved_goal'] = ob['RoundNut_pos']
+         info['is_success'] = reward
+@@ -67,7 +67,7 @@ class NutAssembly(gym.Env):
+         ob = self.env.reset()
+         ob = self.env.observation_spec()
+         observation = {}
+-        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut0_pos'], ob['RoundNut0_quat']))
++        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
+         observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
+         observation['achieved_goal'] = ob['RoundNut_pos']
+         return observation
+diff --git a/requirements.txt b/requirements.txt
+index 700015d..d6e1198 100644
+--- a/requirements.txt
++++ b/requirements.txt
+@@ -1,3 +1 @@
+-cffi==1.14.0
+-PyOpenGL==3.1.5
+-pytest==6.1.2
++-e .
+diff --git a/setup.py b/setup.py
+index 109baa6..f1e4f2c 100644
+--- a/setup.py
++++ b/setup.py
+@@ -23,4 +23,11 @@ setup(
+     eager_resources=["*"],
+     include_package_data=True,
+     python_requires=">=3",
++    description="robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
++    author="Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martín-Martín",
++    url="https://github.com/ARISE-Initiative/robosuite",
++    author_email="yukez@cs.utexas.edu",
++    version="1.3.2",
++    long_description=long_description,
++    long_description_content_type="text/markdown",
+ )
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/events.out.tfevents.1654606564.tamsGPU2.32332.0 b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/events.out.tfevents.1654606564.tamsGPU2.32332.0
new file mode 120000
index 0000000..3a05aa8
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/events.out.tfevents.1654606564.tamsGPU2.32332.0
@@ -0,0 +1 @@
+/homeL/wchen/Assembly_RPL/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/rl_NutAssembly_1_14_56_01-07_06_2022/events.out.tfevents.1654606564.tamsGPU2.32332.0
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/requirements.txt b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/requirements.txt
new file mode 100644
index 0000000..6bdd895
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/requirements.txt
@@ -0,0 +1,103 @@
+absl-py==1.0.0
+asttokens==2.0.5
+attrs==21.4.0
+backcall==0.2.0
+cachetools==5.0.0
+certifi==2021.10.8
+cffi==1.15.0
+charset-normalizer==2.0.12
+click==8.1.3
+cloudpickle==2.0.0
+cython==0.29.28
+decorator==5.1.1
+docker-pycreds==0.4.0
+executing==0.8.3
+fasteners==0.15
+free-mujoco-py==2.1.6
+future==0.18.2
+gitdb==4.0.9
+gitpython==3.1.27
+glfw==1.12.0
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.46.1
+gym-notices==0.0.6
+gym-robotics==0.1.0
+gym==0.23.1
+h5py==3.6.0
+idna==3.3
+imagehash==4.2.1
+imageio-ffmpeg==0.4.7
+imageio==2.19.1
+importlib-metadata==4.11.3
+iniconfig==1.1.1
+ipdb==0.13.9
+ipython==8.3.0
+jedi==0.18.1
+llvmlite==0.36.0
+markdown==3.3.7
+matplotlib-inline==0.1.3
+mkl-fft==1.3.0
+mkl-random==1.1.1
+mkl-service==2.3.0
+monotonic==1.6
+mpi4py==3.1.3
+mujoco-py==2.1.2.14
+numba==0.53.1
+numpy==1.22.3
+oauthlib==3.2.0
+openvr==1.16.802
+packaging==21.3
+parso==0.8.3
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==9.0.1
+pip==21.2.4
+pluggy==1.0.0
+promise==2.3
+prompt-toolkit==3.0.29
+protobuf==3.20.1
+psutil==5.9.0
+ptyprocess==0.7.0
+pure-eval==0.2.2
+py==1.11.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybullet==3.2.5
+pycparser==2.21
+pygments==2.12.0
+pyopengl==3.1.6
+pyparsing==3.0.9
+pytest==7.1.2
+python-dateutil==2.8.2
+pywavelets==1.3.0
+pyyaml==6.0
+requests-oauthlib==1.3.1
+requests==2.27.1
+robosuite==1.3.2
+rsa==4.8
+scipy==1.8.0
+sentry-sdk==1.5.12
+setproctitle==1.2.3
+setuptools==61.2.0
+shortuuid==1.0.9
+six==1.16.0
+smmap==5.0.0
+stack-data==0.2.0
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.9.0
+toml==0.10.2
+tomli==2.0.1
+torch==1.7.1
+torchvision==0.8.0a0
+tqdm==4.64.0
+traitlets==5.2.0
+typing-extensions==4.1.1
+urllib3==1.26.9
+wandb==0.12.16
+wcwidth==0.2.5
+werkzeug==2.1.2
+wheel==0.37.1
+zipp==3.8.0
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/rl_NutAssembly_1_14_56_01-07_06_2022/events.out.tfevents.1654606564.tamsGPU2.32332.0 b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/rl_NutAssembly_1_14_56_01-07_06_2022/events.out.tfevents.1654606564.tamsGPU2.32332.0
new file mode 100644
index 0000000..a380478
Binary files /dev/null and b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/rl_NutAssembly_1_14_56_01-07_06_2022/events.out.tfevents.1654606564.tamsGPU2.32332.0 differ
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/wandb-metadata.json b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/wandb-metadata.json
new file mode 100644
index 0000000..8f5324e
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/wandb-metadata.json
@@ -0,0 +1,33 @@
+{
+    "os": "Linux-5.4.0-100-generic-x86_64-with-glibc2.17",
+    "python": "3.8.13",
+    "heartbeatAt": "2022-06-07T12:56:02.222261",
+    "startedAt": "2022-06-07T12:56:01.450337",
+    "docker": null,
+    "gpu": "NVIDIA GeForce RTX 2080 Ti",
+    "gpu_count": 2,
+    "cpu_count": 24,
+    "cuda": null,
+    "args": [
+        "--env_name",
+        "NutAssembly",
+        "--seed",
+        "1",
+        "--n_epochs",
+        "500",
+        "--exp_name",
+        "rl"
+    ],
+    "state": "running",
+    "program": "RL/ddpg/ddpg.py",
+    "codePath": "Residual-Policy-Learning/RL/ddpg/ddpg.py",
+    "git": {
+        "remote": "https://github.com/turbohiro/Assembly_RPL",
+        "commit": "57769111097c70fc522f3c051ef234a5d053c7f6"
+    },
+    "email": null,
+    "root": "/homeL/wchen/Assembly_RPL",
+    "host": "tamsGPU2",
+    "username": "wchen",
+    "executable": "/homeL/wchen/anaconda3/envs/RPL/bin/python"
+}
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/wandb-summary.json b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/wandb-summary.json
new file mode 100644
index 0000000..8037f0a
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/files/wandb-summary.json
@@ -0,0 +1 @@
+{"global_step": 0, "_timestamp": 1654606576.0992074, "Cycle Losses/Actor Loss": 7.425632953643799, "Cycle Losses/Critic Loss": 3.533857822418213, "_runtime": 15, "_step": 0}
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/run-1wcj97vz.wandb b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/run-1wcj97vz.wandb
new file mode 100644
index 0000000..e9346a7
Binary files /dev/null and b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145601-1wcj97vz/run-1wcj97vz.wandb differ
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py
new file mode 100644
index 0000000..33b7b76
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/code/Residual-Policy-Learning/RL/ddpg/ddpg.py
@@ -0,0 +1,504 @@
+"""
+    DDPG with HER
+"""
+import copy
+import gym
+import argparse
+import torch
+from torch import nn
+from torch import optim
+import os
+from datetime import datetime
+import numpy as np
+from typing import Tuple
+
+from RL.ddpg.models import actor, critic
+from RL.ddpg.replay_buffer import replay_buffer
+from her.her import her_sampler
+
+OPTIMIZERS = {
+    'adam': optim.Adam,
+    'adamax': optim.Adamax,
+    'rmsprop': optim.RMSprop,
+}
+
+LOSS_FN = {
+    'mse': nn.MSELoss(),
+    'smooth_l1': nn.SmoothL1Loss(),
+    'l1': nn.L1Loss()
+}
+
+class DDPG_Agent:
+    def __init__(self, args:argparse.Namespace, env, save_dir: str, device:str, writer=None):
+        """
+            Module for the DDPG agent along with HER
+            Parameters:
+            -----------
+            args: argparse.Namespace
+                args should contain the following:
+            env: gym.Env
+                OpenAI type gym environment
+            save_dir: str
+                Path to save the network weights, checkpoints
+            device: str
+                device to run the training process on
+                Choices: 'cpu', 'cuda'
+            writer: tensorboardX
+                tensorboardX to log metrics like losses, rewards, success_rates, etc.
+        """
+        self.args = args
+        self.env = env
+        self.env_params = self.get_env_params(env)
+        self.save_dir = save_dir
+        self.device = device
+        self.writer = writer
+        self.sim_steps = 0      # a count to keep track of number of simulation steps
+        self.train_steps = 0    # a count to keep track of number of training steps
+
+        # create the network
+        self.actor_network = actor(args, self.env_params).to(device)
+        self.critic_network = critic(args, self.env_params).to(device)
+        # build up the target network
+        self.actor_target_network = actor(args, self.env_params).to(device)
+        self.critic_target_network = critic(args, self.env_params).to(device)
+
+        # load the weights into the target networks
+        self.actor_target_network.load_state_dict(self.actor_network.state_dict())
+        self.critic_target_network.load_state_dict(self.critic_network.state_dict())
+
+        # create the optimizer
+        self.actor_optim  = OPTIMIZERS[args.actor_optim](self.actor_network.parameters(),\
+                                                         lr=self.args.actor_lr,\
+                                                         weight_decay=self.args.weight_decay)
+        self.critic_optim = OPTIMIZERS[args.critic_optim](self.critic_network.parameters(),
+                                                         lr=self.args.critic_lr,\
+                                                         weight_decay=self.args.weight_decay)
+        self.burn_in_done = False   # to check if burn-in is done or not
+
+        # loss function for DDPG
+        self.criterion = LOSS_FN[args.loss_fn]
+
+        # her sampler
+        self.her_module = her_sampler(replay_strategy = self.args.replay_strategy, \
+                                      replay_k = self.args.replay_k, \
+                                      reward_func = self.env.compute_reward)
+        # create the replay buffer
+        self.buffer = replay_buffer(self.env_params, \
+                                    self.args.buffer_size, \
+                                    self.her_module.sample_her_transitions)
+    
+    def get_env_params(self, env):
+        """
+            Get the environment parameters
+        """
+        obs = env.reset()
+        # close the environment
+        params = {'obs': obs['observation'].shape[0],
+                'goal': obs['desired_goal'].shape[0],
+                'action': env.action_space.shape[0],
+                'action_max': env.action_space.high[0],
+                }
+        try:
+            params['max_timesteps'] = env._max_episode_steps
+        # for custom envs
+        except:
+            params['max_timesteps'] = env.max_episode_steps
+        return params
+    
+    def set_actor_lr(self, loss:float, prev_loss:float ,verbose:bool=True):
+        """
+            Set the learning rate of the actor network
+            to either the original learning rate
+            or zero depending on the burn-in parameter
+            diff_loss = |loss - prev_loss|
+            if rl:
+                actor_lr = original_actor_lr
+            if residual learning and diff_loss < beta:
+                actor_lr = original_actor_lr
+            elif residual learning and diff_loss > beta:
+                actor_lr = 0
+            Parameters:
+            -----------
+            loss: float
+                Mean of the losses till the current epoch
+            prev_loss: float
+                Mean of losses till the last epoch
+            verbose: bool
+                To print the change in learning rate
+        """
+        lr = self.args.actor_lr
+        coin_flipping = False
+        # loss is zero only in the first epoch hence do not change lr then
+        # and that's why give a large value to diff_loss
+        diff_loss = 100 if loss == 0 else abs(loss - prev_loss)
+        if not self.burn_in_done:
+            if self.args.exp_name == 'res' and diff_loss > self.args.beta:
+                lr = 0.0
+                coin_flipping = True
+            elif self.args.exp_name == 'res' and diff_loss <= self.args.beta:
+                if verbose:
+                    print('_'*80)
+                    print(f'Burn-in of the critic done. Changing actor_lr from 0.0 to {self.args.actor_lr}')
+                    print('_'*80)
+                self.burn_in_done = True
+
+        for param_group in self.actor_optim.param_groups:
+            param_group['lr'] = lr
+        return coin_flipping
+        
+    def train(self):
+        """
+            Run the episodes for training
+        """
+        print('_'*50)
+        print('Beginning the training...')
+        print('_'*50)
+        num_cycles = 0
+        actor_losses = [0.0]    # to store actor losses for burn-in
+        critic_losses = [0.0]   # to store critic losses for burn-in
+        prev_losses = [0.0]
+        coin_flipping = False   # whether the whole episode should be noise and randomness free
+        deterministic = False   # choose whether we want deterministic or not  
+        for epoch in range(self.args.n_epochs):
+
+            # change the actor learning rate from zero to actor_lr by checking burn-in
+            # check config.py for more information on args.beta_monitor
+            if self.args.beta_monitor == 'actor':
+                coin_flipping = self.set_actor_lr(np.mean(actor_losses), np.mean(prev_losses))
+                prev_losses = actor_losses.copy()
+            elif self.args.beta_monitor == 'critic':
+                coin_flipping = self.set_actor_lr(np.mean(critic_losses), np.mean(prev_losses))
+                prev_losses = critic_losses.copy()
+
+            for cycle in range(self.args.n_cycles):
+
+                ep_obs, ep_ag, ep_g, ep_actions = [], [], [], []
+                # reset the environment
+                observation = self.env.reset()
+                observation_new = copy.deepcopy(observation)
+                obs = observation['observation']
+                ag = observation['achieved_goal']
+                g = observation['desired_goal']
+
+                random_eps = self.args.random_eps
+                noise_eps  = self.args.noise_eps
+                if coin_flipping:
+                    deterministic = np.random.random() < self.args.coin_flipping_prob  # NOTE/TODO change here
+                if deterministic:
+                    random_eps = 0.0
+                    noise_eps = 0.0
+
+                for t in range(self.env_params['max_timesteps']):
+                    # take actions 
+                    with torch.no_grad():
+                        state = self.preprocess_inputs(obs, g)
+                        pi = self.actor_network(state)
+                        if self.args.exp_name == 'res':
+                            controller_action = self.get_controller_actions(observation_new)
+                            action = self.select_actions(pi, noise_eps=noise_eps, random_eps= random_eps, controller_action=controller_action)
+                        else:
+                            action = self.select_actions(pi, noise_eps=noise_eps, random_eps= random_eps, controller_action=None)
+                    # give the action to the environment
+                    observation_new, _, _, info = self.env.step(action)
+                    self.sim_steps += 1                     # increase the simulation timestep by one
+                    obs_new = observation_new['observation']
+                    ag_new = observation_new['achieved_goal']
+                    # append rollouts
+                    ep_obs.append(obs.copy())
+                    ep_ag.append(ag.copy())
+                    ep_g.append(g.copy())
+                    ep_actions.append(action.copy())
+                    # re-assign the observation
+                    obs = obs_new
+                    ag = ag_new
+                # append last states in the array, extend the episode chain (state machine)
+                ep_obs.append(obs.copy())
+                ep_ag.append(ag.copy())
+                # convert to np arrays
+                ep_obs = np.expand_dims(np.array(ep_obs),0)
+                ep_ag = np.expand_dims(np.array(ep_ag),0)
+                ep_g = np.expand_dims(np.array(ep_g),0)
+                ep_actions = np.expand_dims(np.array(ep_actions),0)
+
+                # store them in buffer
+                self.buffer.store_episode([ep_obs, ep_ag, ep_g, ep_actions])
+
+                actor_loss_cycle = 0; critic_loss_cycle = 0 
+                for batch in range(self.args.n_batches):
+                    # train the network with 'n_batches' number of batches
+                    actor_loss, critic_loss = self.update_network(self.train_steps)
+                    self.train_steps += 1
+                    actor_losses.append(actor_loss)
+                    critic_losses.append(critic_loss)
+                    actor_loss_cycle += actor_loss
+                    critic_loss_cycle += critic_loss
+                if self.writer:
+                    self.writer.add_scalar('Cycle Losses/Actor Loss', actor_loss_cycle, num_cycles)
+                    self.writer.add_scalar('Cycle Losses/Critic Loss', critic_loss_cycle, num_cycles)
+                self.polyak_update_networks(self.actor_target_network, self.actor_network)
+                self.polyak_update_networks(self.critic_target_network, self.critic_network)
+                num_cycles += 1
+            # evaluate the agent
+            success_rate = self.eval_agent()
+            print(f'Epoch Critic: {np.mean(critic_losses):.3f} Epoch Actor:{np.mean(actor_losses):.3f}')
+            print(f'Epoch:{epoch}\tSuccess Rate:{success_rate:.3f}')
+            if self.writer:
+                self.writer.add_scalar('Success Rate/Success Rate', success_rate, self.sim_steps)
+                self.writer.add_scalar('Epoch Losses/Average Critic Loss', np.mean(critic_losses), epoch)
+                self.writer.add_scalar('Epoch Losses/Average Actor Loss', np.mean(actor_losses), epoch)
+            
+            self.save_checkpoint(self.save_dir)
+
+    def preprocess_inputs(self, obs:np.ndarray, g:np.ndarray) -> torch.Tensor:
+        """
+            Concatenate state and goal
+            and convert them to torch tensors
+            and then transfer them to either CPU of GPU
+        """
+        # concatenate the stuffs
+        inputs = np.concatenate([obs, g])   #observation input (state) and goal label (ground truth)
+        inputs = torch.tensor(inputs, dtype=torch.float32).unsqueeze(0)
+        inputs = inputs.to(self.device)
+        return inputs
+    
+    def get_controller_actions(self, obs:dict):
+        """
+            Return the controller action if residual learning
+        """
+        return self.env.controller_action(obs, take_action=False)
+
+    def select_actions(self, pi: torch.Tensor, noise_eps:float, random_eps:float, controller_action=None) -> np.ndarray:
+        """
+            Take action
+            with a probability of self.args.random_eps, it will take random actions
+            otherwise, this will add a gaussian noise to the action along with clipping
+            pi: torch.Tensor
+                The action given by the actor
+            noise_eps: float
+                The random gaussian noise added to actor output
+            random_eps: float
+                Probability of taking random action
+            controller_action: None or np.ndarray
+                To subtract from random action
+        """
+        # transfer action from CUDA to CPU if using GPU and make numpy array out of it
+        action = pi.cpu().numpy().squeeze()
+        # add the gaussian
+        action += noise_eps * self.env_params['action_max'] * np.random.randn(*action.shape)
+        action = np.clip(action, -self.env_params['action_max'], self.env_params['action_max'])  #make action values are limited a feasible range
+
+        # random actions
+        random_actions = np.random.uniform(low=-self.env_params['action_max'], high=self.env_params['action_max'], \
+                                            size=self.env_params['action'])
+        # if residual learning, subtract the controller action so that we don't add it twice
+        if self.args.exp_name == 'res':
+            random_actions = random_actions - controller_action
+        # choose whether to take random actions or not
+        rand = np.random.binomial(1, random_eps, 1)[0]
+        action += rand * (random_actions - action)  # will be equal to either random_actions or action
+        return action
+
+    def preprocess_og(self, o:np.ndarray, g:np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
+        """
+            Perform observation clipping
+        """
+        o = np.clip(o, -self.args.clip_obs, self.args.clip_obs)
+        g = np.clip(g, -self.args.clip_obs, self.args.clip_obs)
+        return o, g
+
+    def polyak_update_networks(self, target, source):
+        """
+            Polyak averaging of target and main networks; Also known as soft update of networks
+            target_net_params = (1 - polyak) * main_net_params + polyak * target_net_params
+        """
+        for target_param, param in zip(target.parameters(), source.parameters()):
+            target_param.data.copy_((1 - self.args.polyak) * param.data + self.args.polyak * target_param.data)
+
+    def update_network(self, step:int) -> Tuple[float, float]:
+        """
+            The actual DDPG training
+        """
+
+        # sample the episodes
+        transitions = self.buffer.sample(self.args.batch_size)
+
+        # pre-process the observation and goal
+        o, o_next, g = transitions['obs'], transitions['obs_next'], transitions['g']
+        transitions['obs'], transitions['g'] = self.preprocess_og(o, g)
+        transitions['obs_next'], transitions['g_next'] = self.preprocess_og(o_next, g)
+
+        # concatenate obs and goal
+        states = np.concatenate([transitions['obs'], transitions['g']], axis=1)
+        next_states = np.concatenate([transitions['obs_next'], transitions['g_next']], axis=1)
+
+        # convert to tensor
+        states = torch.tensor(states, dtype=torch.float32).to(self.device)
+        next_states = torch.tensor(next_states, dtype=torch.float32).to(self.device)
+        actions = torch.tensor(transitions['actions'], dtype=torch.float32).to(self.device)
+        rewards = torch.tensor(transitions['r'], dtype=torch.float32).to(self.device)
+
+        with torch.no_grad():
+            actions_next = self.actor_target_network(next_states)
+            q_next_value = self.critic_target_network(next_states, actions_next)
+            q_next_value = q_next_value.detach()
+            target_q_value = rewards + self.args.gamma * q_next_value
+            target_q_value = target_q_value.detach()
+            # clip the returns
+            clip_return = 1 / (1-self.args.gamma)
+            target_q_value = torch.clamp(target_q_value, -clip_return, 0)
+        
+        # critic loss
+        q_value = self.critic_network(states, actions)
+        critic_loss = self.criterion(target_q_value, q_value)   # loss (mostly MSE)
+
+        # actor loss
+        actions_pred = self.actor_network(states)
+        actor_loss = -self.critic_network(states, actions_pred).mean()   #the max of q values, the better of actor(policy)
+        actor_loss = actor_loss + self.args.action_l2 * (actions_pred / self.env_params['action_max']).pow(2).mean()
+
+        # backpropagate
+        self.actor_optim.zero_grad()    # zero the gradients
+        actor_loss.backward()           # backward prop
+        self.actor_optim.step()         # take step towards gradient direction
+
+        self.critic_optim.zero_grad()    # zero the gradients
+        critic_loss.backward()           # backward prop
+        self.critic_optim.step()         # take step towards gradient directions
+
+        return actor_loss.item(), critic_loss.item()
+        
+    def eval_agent(self) -> float:
+        """
+            Evaluate the agent using the trained policy
+            performs n_test_rollouts in the environment
+            and returns
+        """
+        successes = []
+        for _ in range(self.args.n_test_rollouts):
+            success = np.zeros(self.env_params['max_timesteps'])
+            observation = self.env.reset()
+            obs = observation['observation']
+            g = observation['desired_goal']
+            for i in range(self.env_params['max_timesteps']):
+                with torch.no_grad():
+                    input_tensor = self.preprocess_inputs(obs,g)
+                    pi = self.actor_network(input_tensor)
+                    actions = pi.detach().cpu().numpy().squeeze()
+                observation_new, _, _, info = self.env.step(actions)
+                obs = observation_new['observation']
+                g = observation_new['desired_goal']
+                success[i] = info['is_success']
+            successes.append(success)
+        successes = np.array(successes)
+        return np.mean(successes[:,-1]) # return mean of only final steps success
+
+    def save_checkpoint(self, path:str):
+        """
+            Saves the model in the wandb experiment run directory
+            This will store the 
+                * model state_dict
+                * optimizer state_dict
+                * args/hparams
+            param:
+                path: str
+                    path to the wandb run directory
+                    Example: os.path.join(wandb.run.dir, "model.ckpt")
+        """
+        checkpoint = {}
+        checkpoint['args'] = vars(self.args)
+        checkpoint['actor_state_dict'] = self.actor_network.state_dict()
+        checkpoint['env_name'] = self.args.env_name
+        torch.save(checkpoint, path)
+
+    def load_checkpoint(self, path:str):
+        """
+            Load the trained model weights
+            param:
+                path: str
+                    path to the saved weights file
+        """
+        use_cuda = torch.cuda.is_available()
+        device   = torch.device("cuda" if use_cuda else "cpu")
+        checkpoint_dict = torch.load(path, map_location=device)
+        self.actor_network.load_state_dict(checkpoint_dict['actor_state_dict'])
+
+if __name__ == "__main__":
+    # run from main folder as `python RL/ddpg.py`
+    import time
+    import wandb
+    from torch.utils.tensorboard import SummaryWriter
+
+    from ddpg_config import args
+    from utils import connected_to_internet, make_env, get_pretty_env_name
+    import pdb
+
+    # check whether GPU is available or not
+    use_cuda = torch.cuda.is_available()
+    device = torch.device("cuda" if use_cuda else "cpu")
+    print('_'*50)
+    print('Device:',device)
+    print('_'*50)
+    args.device = device
+    args.mpi = False        # not running on mpi mode
+    #####################################
+
+    env = make_env(args.env_name)   # initialise the environment
+    #env = gym.make(args.env_name)   # initialise the environment
+    
+    # set the random seeds for everything
+    # random.seed(args.seed)
+    np.random.seed(args.seed)
+    torch.manual_seed(args.seed)
+    torch.backends.cudnn.deterministic = args.torch_deterministic
+    env.seed(args.seed)
+    env.action_space.seed(args.seed)
+    env.observation_space.seed(args.seed)
+    #####################################
+
+    # book keeping to log stuff
+    if args.dryrun:
+        writer = None
+        weight_save_path = 'model_dryrun.ckpt'
+    else:
+        # check internet connection
+        # for offline wandb. Will load everything on cloud afterwards
+        if not connected_to_internet():
+            import json
+            # save a json file with your wandb api key in your home folder as {'my_wandb_api_key': 'INSERT API HERE'}
+            # NOTE this is only for running on MIT Supercloud
+            with open(os.path.expanduser('~')+'/keys.json') as json_file: 
+                key = json.load(json_file)
+                my_wandb_api_key = key['my_wandb_api_key'] # NOTE change here as well
+            os.environ["WANDB_API_KEY"] = my_wandb_api_key # my Wandb api key
+            os.environ["WANDB_MODE"] = "dryrun"
+
+        start_time = time.strftime("%H_%M_%S-%d_%m_%Y", time.localtime())
+        pretty_env_name = get_pretty_env_name(args.env_name)
+        experiment_name = f"{args.exp_name}_{pretty_env_name}_{args.seed}_{start_time}"
+            
+        print('_'*50)
+        print('Creating wandboard...')
+        print('_'*50)
+        wandb_save_dir = os.path.join(os.path.abspath(os.getcwd()),f"wandb_{pretty_env_name}")
+        if not os.path.exists(wandb_save_dir):
+            os.makedirs(wandb_save_dir)
+        wandb.init(project='Residual Policy Learning', entity='turbohiro',\
+                   sync_tensorboard=True, config=vars(args), name=experiment_name,\
+                   save_code=True, dir=wandb_save_dir, group=f"{pretty_env_name}")
+        writer = SummaryWriter(f"{wandb.run.dir}/{experiment_name}")
+        weight_save_path = os.path.join(wandb.run.dir, "model.ckpt")
+    ##########################################################################
+    print('_'*50)
+    print('Arguments:')
+    for arg in vars(args):
+        print(f'{arg} = {getattr(args, arg)}')
+    print('_'*50)
+    # initialise the agent
+    trainer = DDPG_Agent(args, env, save_dir=weight_save_path, device=device, writer=writer)
+    # train the agent
+    trainer.train()
+
+    # close env and writer
+    env.close()
+    if writer:
+        writer.close()
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/conda-environment.yaml b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/conda-environment.yaml
new file mode 100644
index 0000000..e8dbed5
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/conda-environment.yaml
@@ -0,0 +1,142 @@
+name: RPL
+channels:
+  - defaults
+dependencies:
+  - _libgcc_mutex=0.1=main
+  - _openmp_mutex=4.5=1_gnu
+  - _pytorch_select=0.1=cpu_0
+  - blas=1.0=mkl
+  - ca-certificates=2022.4.26=h06a4308_0
+  - certifi=2021.10.8=py38h06a4308_2
+  - cffi=1.15.0=py38hd667e15_1
+  - freetype=2.11.0=h70c0345_0
+  - future=0.18.2=py38_1
+  - giflib=5.2.1=h7b6447c_0
+  - intel-openmp=2019.4=243
+  - jpeg=9e=h7f8727e_0
+  - lcms2=2.12=h3be6417_0
+  - ld_impl_linux-64=2.35.1=h7274673_9
+  - libffi=3.3=he6710b0_2
+  - libgcc-ng=9.3.0=h5101ec6_17
+  - libgomp=9.3.0=h5101ec6_17
+  - libmklml=2019.0.5=h06a4308_0
+  - libpng=1.6.37=hbc83047_0
+  - libstdcxx-ng=9.3.0=hd4cf53a_17
+  - libtiff=4.2.0=h85742a9_0
+  - libwebp=1.2.2=h55f646e_0
+  - libwebp-base=1.2.2=h7f8727e_0
+  - lz4-c=1.9.3=h295c915_1
+  - mkl=2020.2=256
+  - mkl-service=2.3.0=py38he904b0f_0
+  - mkl_fft=1.3.0=py38h54f3939_0
+  - mkl_random=1.1.1=py38h0573a6f_0
+  - ncurses=6.3=h7f8727e_2
+  - ninja=1.10.2=h06a4308_5
+  - ninja-base=1.10.2=hd09550d_5
+  - openssl=1.1.1o=h7f8727e_0
+  - pillow=9.0.1=py38h22f2fdc_0
+  - pip=21.2.4=py38h06a4308_0
+  - pycparser=2.21=pyhd3eb1b0_0
+  - python=3.8.13=h12debd9_0
+  - pytorch=1.7.1=cpu_py38h6a09485_0
+  - readline=8.1.2=h7f8727e_1
+  - setuptools=61.2.0=py38h06a4308_0
+  - six=1.16.0=pyhd3eb1b0_1
+  - sqlite=3.38.3=hc218d9a_0
+  - tk=8.6.11=h1ccaba5_1
+  - torchvision=0.8.2=cpu_py38ha229d99_0
+  - typing-extensions=4.1.1=hd3eb1b0_0
+  - typing_extensions=4.1.1=pyh06a4308_0
+  - wheel=0.37.1=pyhd3eb1b0_0
+  - xz=5.2.5=h7f8727e_1
+  - zlib=1.2.12=h7f8727e_2
+  - zstd=1.4.9=haebb681_0
+  - pip:
+    - absl-py==1.0.0
+    - asttokens==2.0.5
+    - attrs==21.4.0
+    - backcall==0.2.0
+    - cachetools==5.0.0
+    - charset-normalizer==2.0.12
+    - click==8.1.3
+    - cloudpickle==2.0.0
+    - cython==0.29.28
+    - decorator==5.1.1
+    - docker-pycreds==0.4.0
+    - executing==0.8.3
+    - fasteners==0.15
+    - free-mujoco-py==2.1.6
+    - gitdb==4.0.9
+    - gitpython==3.1.27
+    - glfw==1.12.0
+    - google-auth==2.6.6
+    - google-auth-oauthlib==0.4.6
+    - grpcio==1.46.1
+    - gym==0.23.1
+    - gym-notices==0.0.6
+    - gym-robotics==0.1.0
+    - h5py==3.6.0
+    - idna==3.3
+    - imagehash==4.2.1
+    - imageio==2.19.1
+    - imageio-ffmpeg==0.4.7
+    - importlib-metadata==4.11.3
+    - iniconfig==1.1.1
+    - ipdb==0.13.9
+    - ipython==8.3.0
+    - jedi==0.18.1
+    - llvmlite==0.36.0
+    - markdown==3.3.7
+    - matplotlib-inline==0.1.3
+    - monotonic==1.6
+    - mpi4py==3.1.3
+    - mujoco-py==2.1.2.14
+    - numba==0.53.1
+    - numpy==1.22.3
+    - oauthlib==3.2.0
+    - openvr==1.16.802
+    - packaging==21.3
+    - parso==0.8.3
+    - pathtools==0.1.2
+    - pexpect==4.8.0
+    - pickleshare==0.7.5
+    - pluggy==1.0.0
+    - promise==2.3
+    - prompt-toolkit==3.0.29
+    - protobuf==3.20.1
+    - psutil==5.9.0
+    - ptyprocess==0.7.0
+    - pure-eval==0.2.2
+    - py==1.11.0
+    - pyasn1==0.4.8
+    - pyasn1-modules==0.2.8
+    - pybullet==3.2.5
+    - pygments==2.12.0
+    - pyopengl==3.1.6
+    - pyparsing==3.0.9
+    - pytest==7.1.2
+    - python-dateutil==2.8.2
+    - pywavelets==1.3.0
+    - pyyaml==6.0
+    - requests==2.27.1
+    - requests-oauthlib==1.3.1
+    - rsa==4.8
+    - scipy==1.8.0
+    - sentry-sdk==1.5.12
+    - setproctitle==1.2.3
+    - shortuuid==1.0.9
+    - smmap==5.0.0
+    - stack-data==0.2.0
+    - tensorboard==2.9.0
+    - tensorboard-data-server==0.6.1
+    - tensorboard-plugin-wit==1.8.1
+    - toml==0.10.2
+    - tomli==2.0.1
+    - tqdm==4.64.0
+    - traitlets==5.2.0
+    - urllib3==1.26.9
+    - wandb==0.12.16
+    - wcwidth==0.2.5
+    - werkzeug==2.1.2
+    - zipp==3.8.0
+prefix: /homeL/wchen/anaconda3/envs/RPL
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/config.yaml b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/config.yaml
new file mode 100644
index 0000000..57c7dce
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/config.yaml
@@ -0,0 +1,132 @@
+wandb_version: 1
+
+_wandb:
+  desc: null
+  value:
+    cli_version: 0.12.16
+    code_path: code/Residual-Policy-Learning/RL/ddpg/ddpg.py
+    framework: torch
+    is_jupyter_run: false
+    is_kaggle_kernel: false
+    python_version: 3.8.13
+    start_time: 1654606612
+    t:
+      1:
+      - 1
+      - 55
+      3:
+      - 13
+      - 16
+      - 35
+      4: 3.8.13
+      5: 0.12.16
+      8:
+      - 5
+action_l2:
+  desc: null
+  value: 1.0
+activation:
+  desc: null
+  value: relu
+actor_lr:
+  desc: null
+  value: 0.001
+actor_optim:
+  desc: null
+  value: adam
+batch_size:
+  desc: null
+  value: 256
+beta:
+  desc: null
+  value: 1.0
+beta_monitor:
+  desc: null
+  value: critic
+buffer_size:
+  desc: null
+  value: 1000000
+clip_obs:
+  desc: null
+  value: 200
+clip_range:
+  desc: null
+  value: 5
+coin_flipping_prob:
+  desc: null
+  value: 0.5
+critic_lr:
+  desc: null
+  value: 0.001
+critic_optim:
+  desc: null
+  value: adam
+device:
+  desc: null
+  value: cpu
+dryrun:
+  desc: null
+  value: false
+env_name:
+  desc: null
+  value: NutAssemblyHand
+exp_name:
+  desc: null
+  value: res
+gamma:
+  desc: null
+  value: 0.98
+hidden_dims:
+  desc: null
+  value:
+  - 256
+  - 256
+  - 256
+loss_fn:
+  desc: null
+  value: mse
+max_grad_norm:
+  desc: null
+  value: 0.5
+mpi:
+  desc: null
+  value: false
+n_batches:
+  desc: null
+  value: 40
+n_cycles:
+  desc: null
+  value: 50
+n_epochs:
+  desc: null
+  value: 500
+n_test_rollouts:
+  desc: null
+  value: 50
+noise_eps:
+  desc: null
+  value: 0.2
+num_rollouts_per_mpi:
+  desc: null
+  value: 2
+polyak:
+  desc: null
+  value: 0.95
+random_eps:
+  desc: null
+  value: 0.3
+replay_k:
+  desc: null
+  value: 4
+replay_strategy:
+  desc: null
+  value: future
+seed:
+  desc: null
+  value: 1
+torch_deterministic:
+  desc: null
+  value: true
+weight_decay:
+  desc: null
+  value: 0.0
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/diff.patch b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/diff.patch
new file mode 100644
index 0000000..516d4a9
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/diff.patch
@@ -0,0 +1,47 @@
+diff --git a/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py b/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
+index 8d63c3f..0e95116 100644
+--- a/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
++++ b/Residual-Policy-Learning/controllers/robosuite/robosuiteNutAssemblyEnv.py
+@@ -57,7 +57,7 @@ class NutAssembly(gym.Env):
+         ob, reward, done, info = self.env.step(action)
+         ob = self.env.observation_spec()
+         observation = {}
+-        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut0_pos'], ob['RoundNut0_quat']))
++        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
+         observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
+         observation['achieved_goal'] = ob['RoundNut_pos']
+         info['is_success'] = reward
+@@ -67,7 +67,7 @@ class NutAssembly(gym.Env):
+         ob = self.env.reset()
+         ob = self.env.observation_spec()
+         observation = {}
+-        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut0_pos'], ob['RoundNut0_quat']))
++        observation['observation'] = np.hstack((ob['robot0_eef_pos'], ob['robot0_eef_quat'], ob['RoundNut_pos'], ob['RoundNut_quat']))
+         observation['desired_goal'] = np.array(self.env.sim.data.body_xpos[self.env.peg2_body_id])
+         observation['achieved_goal'] = ob['RoundNut_pos']
+         return observation
+diff --git a/requirements.txt b/requirements.txt
+index 700015d..d6e1198 100644
+--- a/requirements.txt
++++ b/requirements.txt
+@@ -1,3 +1 @@
+-cffi==1.14.0
+-PyOpenGL==3.1.5
+-pytest==6.1.2
++-e .
+diff --git a/setup.py b/setup.py
+index 109baa6..f1e4f2c 100644
+--- a/setup.py
++++ b/setup.py
+@@ -23,4 +23,11 @@ setup(
+     eager_resources=["*"],
+     include_package_data=True,
+     python_requires=">=3",
++    description="robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
++    author="Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martín-Martín",
++    url="https://github.com/ARISE-Initiative/robosuite",
++    author_email="yukez@cs.utexas.edu",
++    version="1.3.2",
++    long_description=long_description,
++    long_description_content_type="text/markdown",
+ )
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/events.out.tfevents.1654606614.tamsGPU2.1829.0 b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/events.out.tfevents.1654606614.tamsGPU2.1829.0
new file mode 120000
index 0000000..5f5b3c7
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/events.out.tfevents.1654606614.tamsGPU2.1829.0
@@ -0,0 +1 @@
+/homeL/wchen/Assembly_RPL/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/res_NutAssembly_1_14_56_51-07_06_2022/events.out.tfevents.1654606614.tamsGPU2.1829.0
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/requirements.txt b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/requirements.txt
new file mode 100644
index 0000000..6bdd895
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/requirements.txt
@@ -0,0 +1,103 @@
+absl-py==1.0.0
+asttokens==2.0.5
+attrs==21.4.0
+backcall==0.2.0
+cachetools==5.0.0
+certifi==2021.10.8
+cffi==1.15.0
+charset-normalizer==2.0.12
+click==8.1.3
+cloudpickle==2.0.0
+cython==0.29.28
+decorator==5.1.1
+docker-pycreds==0.4.0
+executing==0.8.3
+fasteners==0.15
+free-mujoco-py==2.1.6
+future==0.18.2
+gitdb==4.0.9
+gitpython==3.1.27
+glfw==1.12.0
+google-auth-oauthlib==0.4.6
+google-auth==2.6.6
+grpcio==1.46.1
+gym-notices==0.0.6
+gym-robotics==0.1.0
+gym==0.23.1
+h5py==3.6.0
+idna==3.3
+imagehash==4.2.1
+imageio-ffmpeg==0.4.7
+imageio==2.19.1
+importlib-metadata==4.11.3
+iniconfig==1.1.1
+ipdb==0.13.9
+ipython==8.3.0
+jedi==0.18.1
+llvmlite==0.36.0
+markdown==3.3.7
+matplotlib-inline==0.1.3
+mkl-fft==1.3.0
+mkl-random==1.1.1
+mkl-service==2.3.0
+monotonic==1.6
+mpi4py==3.1.3
+mujoco-py==2.1.2.14
+numba==0.53.1
+numpy==1.22.3
+oauthlib==3.2.0
+openvr==1.16.802
+packaging==21.3
+parso==0.8.3
+pathtools==0.1.2
+pexpect==4.8.0
+pickleshare==0.7.5
+pillow==9.0.1
+pip==21.2.4
+pluggy==1.0.0
+promise==2.3
+prompt-toolkit==3.0.29
+protobuf==3.20.1
+psutil==5.9.0
+ptyprocess==0.7.0
+pure-eval==0.2.2
+py==1.11.0
+pyasn1-modules==0.2.8
+pyasn1==0.4.8
+pybullet==3.2.5
+pycparser==2.21
+pygments==2.12.0
+pyopengl==3.1.6
+pyparsing==3.0.9
+pytest==7.1.2
+python-dateutil==2.8.2
+pywavelets==1.3.0
+pyyaml==6.0
+requests-oauthlib==1.3.1
+requests==2.27.1
+robosuite==1.3.2
+rsa==4.8
+scipy==1.8.0
+sentry-sdk==1.5.12
+setproctitle==1.2.3
+setuptools==61.2.0
+shortuuid==1.0.9
+six==1.16.0
+smmap==5.0.0
+stack-data==0.2.0
+tensorboard-data-server==0.6.1
+tensorboard-plugin-wit==1.8.1
+tensorboard==2.9.0
+toml==0.10.2
+tomli==2.0.1
+torch==1.7.1
+torchvision==0.8.0a0
+tqdm==4.64.0
+traitlets==5.2.0
+typing-extensions==4.1.1
+urllib3==1.26.9
+wandb==0.12.16
+wcwidth==0.2.5
+werkzeug==2.1.2
+wheel==0.37.1
+zipp==3.8.0
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/res_NutAssembly_1_14_56_51-07_06_2022/events.out.tfevents.1654606614.tamsGPU2.1829.0 b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/res_NutAssembly_1_14_56_51-07_06_2022/events.out.tfevents.1654606614.tamsGPU2.1829.0
new file mode 100644
index 0000000..a6af149
Binary files /dev/null and b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/res_NutAssembly_1_14_56_51-07_06_2022/events.out.tfevents.1654606614.tamsGPU2.1829.0 differ
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/wandb-metadata.json b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/wandb-metadata.json
new file mode 100644
index 0000000..f4bbc29
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/wandb-metadata.json
@@ -0,0 +1,33 @@
+{
+    "os": "Linux-5.4.0-100-generic-x86_64-with-glibc2.17",
+    "python": "3.8.13",
+    "heartbeatAt": "2022-06-07T12:56:53.027345",
+    "startedAt": "2022-06-07T12:56:52.213679",
+    "docker": null,
+    "gpu": "NVIDIA GeForce RTX 2080 Ti",
+    "gpu_count": 2,
+    "cpu_count": 24,
+    "cuda": null,
+    "args": [
+        "--env_name",
+        "NutAssemblyHand",
+        "--seed",
+        "1",
+        "--n_epochs",
+        "500",
+        "--exp_name",
+        "res"
+    ],
+    "state": "running",
+    "program": "RL/ddpg/ddpg.py",
+    "codePath": "Residual-Policy-Learning/RL/ddpg/ddpg.py",
+    "git": {
+        "remote": "https://github.com/turbohiro/Assembly_RPL",
+        "commit": "57769111097c70fc522f3c051ef234a5d053c7f6"
+    },
+    "email": null,
+    "root": "/homeL/wchen/Assembly_RPL",
+    "host": "tamsGPU2",
+    "username": "wchen",
+    "executable": "/homeL/wchen/anaconda3/envs/RPL/bin/python"
+}
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/wandb-summary.json b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/wandb-summary.json
new file mode 100644
index 0000000..0fb7021
--- /dev/null
+++ b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/files/wandb-summary.json
@@ -0,0 +1 @@
+{"global_step": 0, "_timestamp": 1654606624.486189, "Cycle Losses/Actor Loss": 5.622323036193848, "Cycle Losses/Critic Loss": 2.2765541076660156, "_runtime": 12, "_step": 0}
\ No newline at end of file
diff --git a/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/run-xa3jb4rv.wandb b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/run-xa3jb4rv.wandb
new file mode 100644
index 0000000..88c42d6
Binary files /dev/null and b/Residual-Policy-Learning/wandb_NutAssembly/wandb/run-20220607_145652-xa3jb4rv/run-xa3jb4rv.wandb differ
diff --git a/requirements.txt b/requirements.txt
index 700015d..d6e1198 100644
--- a/requirements.txt
+++ b/requirements.txt
@@ -1,3 +1 @@
-cffi==1.14.0
-PyOpenGL==3.1.5
-pytest==6.1.2
+-e .
diff --git a/setup.py b/setup.py
index 109baa6..f1e4f2c 100644
--- a/setup.py
+++ b/setup.py
@@ -23,4 +23,11 @@ setup(
     eager_resources=["*"],
     include_package_data=True,
     python_requires=">=3",
+    description="robosuite: A Modular Simulation Framework and Benchmark for Robot Learning",
+    author="Yuke Zhu, Josiah Wong, Ajay Mandlekar, Roberto Martín-Martín",
+    url="https://github.com/ARISE-Initiative/robosuite",
+    author_email="yukez@cs.utexas.edu",
+    version="1.3.2",
+    long_description=long_description,
+    long_description_content_type="text/markdown",
 )
